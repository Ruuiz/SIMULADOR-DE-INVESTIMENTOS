{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TICKERS = [\n",
    "    'AALR3', 'ABCB3', 'ABCB4', 'ABEV3', 'ABRE3', 'ACES3', 'ACES4', 'ADHM3', 'AEDU11', 'AEDU3', 'AELP3', 'AERI3', 'AESL3', 'AESL4', 'AFLT3', 'AFLU3', 'AFLU5', 'AGRO3', 'AGXY3', 'AHEB3',\n",
    "    'AHEB5', 'AHEB6', 'ALBA3', 'ALLD3', 'ALLL11', 'ALLL3', 'ALLL4', 'ALOS3', 'ALPA3', 'ALPA4', 'ALPK3', 'ALUP11', 'ALUP3', 'ALUP4', 'AMAR3', 'AMBP3', 'AMBV3', 'AMBV4', 'AMER3', 'AMIL3',\n",
    "    'AMOB3', 'AMPI3', 'ANIM3', 'AORE3', 'APTI4', 'ARCZ3', 'ARCZ6', 'ARLA3', 'ARLA4', 'ARML3', 'ARTE3', 'ARTE4', 'ARTR3', 'ASAI3', 'ASSM3', 'ASSM4', 'ASTA4', 'ATED3', 'ATMP3', 'AURA33',\n",
    "    'AURE3', 'AUTM3', 'AVIL3', 'AVLL3', 'AZEV3', 'AZEV4', 'AZTE3', 'AZUL4', 'AZZA3', 'B3SA3', 'BAHI11', 'BAHI4', 'BAHI5', 'BALM3', 'BALM4', 'BAUH4', 'BAZA3', 'BBAS3', 'BBDC3', 'BBDC4',\n",
    "    'BBSE3', 'BBTG11', 'BCAL6', 'BDLL3', 'BDLL4', 'BECE3', 'BECE4', 'BEEF3', 'BEES3', 'BEES4', 'BELG3', 'BELG4', 'BEMA3', 'BERG3', 'BESP3', 'BESP4', 'BFIT3', 'BFIT4', 'BGIP3', 'BGIP4',\n",
    "    'BHGR3', 'BHIA3', 'BICB3', 'BICB4', 'BIED3', 'BIOM3', 'BIOM4', 'BISA3', 'BLAU3', 'BMEB3', 'BMEB4', 'BMGB4', 'BMIN3', 'BMIN4', 'BMKS3', 'BMOB3', 'BMTO3', 'BMTO4', 'BNBR3', 'BNBR4',\n",
    "    'BOBR3', 'BOBR4', 'BPAC11', 'BPAC3', 'BPAC5', 'BPAN4', 'BPAR3', 'BPHA3', 'BPIA3', 'BPNM4', 'BRAP3', 'BRAP4', 'BRAV3', 'BRBI11', 'BRFS3', 'BRGE11', 'BRGE12', 'BRGE3', 'BRGE5', 'BRGE6',\n",
    "    'BRGE7', 'BRGE8', 'BRIN3', 'BRIV3', 'BRIV4', 'BRKM3', 'BRKM5', 'BRKM6', 'BRPR3', 'BRSR3', 'BRSR4', 'BRSR5', 'BRSR6', 'BRST3', 'BRTP3', 'BRTP4', 'BSCT3', 'BSCT5', 'BSCT6', 'BSGR3',\n",
    "    'BSLI3', 'BSLI4', 'BUET3', 'BUET4', 'BVMF3', 'CAFE3', 'CAFE4', 'CALI3', 'CAMB3', 'CASH3', 'CASN3', 'CBAV3', 'CBEE3', 'CBMA3', 'CBMA4', 'CCHI3', 'CCHI4', 'CCTU4', 'CCTY3', 'CEAB3',\n",
    "    'CEBR3', 'CEBR5', 'CEBR6', 'CEDO3', 'CEDO4', 'CEEB3', 'CEEB5', 'CEED3', 'CEED4', 'CEGR3', 'CELM3', 'CESP4', 'CFLU4', 'CGAS3', 'CGAS5', 'CGOS3', 'CGOS4', 'CGRA3', 'CGRA4', 'CIQU3',\n",
    "    'CIQU4', 'CLSC3', 'CLSC4', 'CLSC5', 'CLSC6', 'CMET4', 'CMIG3', 'CMIG4', 'CMIN3', 'CMMA4', 'CNFB4', 'COCE3', 'COCE5', 'COCE6', 'COGN3', 'CORR3', 'CORR4', 'CPFE3', 'CPFG3', 'CPFG4',\n",
    "    'CPFP4', 'CPLE3', 'CPLE5', 'CPLE6', 'CPSL3', 'CRBM3', 'CRBM7', 'CREM3', 'CREM4', 'CRFB3', 'CRIV3', 'CRIV4', 'CRPG3', 'CRPG5', 'CRPG6', 'CRTP3', 'CRTP5', 'CRUZ3', 'CSAN3', 'CSED3',\n",
    "    'CSMG3', 'CSNA3', 'CSPC3', 'CSPC4', 'CSTB3', 'CSTB4', 'CSUD3', 'CTIP3', 'CTKA3', 'CTKA4', 'CTNM3', 'CTNM4', 'CTPC4', 'CTSA3', 'CTSA4', 'CTSA8', 'CTWR3', 'CURY3', 'CVCB3', 'CXSE3',\n",
    "    'CYRE3', 'CYRE4', 'CZLT33', 'CZRS4', 'DAGB33', 'DASA3', 'DAYC4', 'DESK3', 'DEXP3', 'DEXP4', 'DFVA3', 'DFVA4', 'DHBI3', 'DHBI4', 'DIRR3', 'DJON4', 'DMVF3', 'DOCA3', 'DOCA4', 'DOHL3', \n",
    "    'DOHL4', 'DOTZ3', 'DPPI3', 'DPPI4', 'DTCY3', 'DUQE3', 'DUQE4', 'DURA3', 'DURA4', 'DXCO3', 'DXTG4', 'EALT3', 'EALT4', 'EBCO3', 'EBCO4', 'EBEN4', 'EBTP3', 'EBTP4', 'ECIS3', 'ECIS4',\n",
    "    'ECOR3', 'ECPR3', 'ECPR4', 'EGIE3', 'EKTR3', 'EKTR4', 'ELCA3', 'ELCA4', 'ELET3', 'ELET5', 'ELET6', 'ELMD3', 'ELPL4', 'ELUM3', 'ELUM4', 'EMAE4', 'EMBR3', 'ENER3', 'ENER5', 'ENER6',\n",
    "    'ENEV3', 'ENGI11', 'ENGI3', 'ENGI4', 'ENJU3', 'ENMT3', 'ENMT4', 'EPAR3', 'EQMA3B', 'EQPA3', 'EQPA5', 'EQPA6', 'EQPA7', 'EQTL3', 'ESCE3', 'ESPA3', 'ESTR3', 'ESTR4', 'ETER3', 'ETER4',\n",
    "    'EUCA3', 'EUCA4', 'EVEN3', 'EZTC3', 'FBMC3', 'FBMC4', 'FBRA4', 'FCAP3', 'FCAP4', 'FESA3', 'FESA4', 'FFTL3', 'FFTL4', 'FGUI3', 'FGUI4', 'FHER3', 'FIBR3', 'FICT3', 'FIEI3', 'FIGE3',\n",
    "    'FIGE4', 'FIQE3', 'FLCL3', 'FLCL5', 'FLCL6', 'FLRY3', 'FRAS3', 'FRAS4', 'FRIO3', 'FTRX3', 'FTRX4', 'GAFP3', 'GAFP4', 'GALO4', 'GAZO4', 'GEPA3', 'GEPA4', 'GETI3', 'GETI4', 'GFSA3',\n",
    "    'GGBR3', 'GGBR4', 'GGPS3', 'GLOB4', 'GMAT3', 'GOAU3', 'GOAU4', 'GOLL4', 'GPAR3', 'GPIV33', 'GRND3', 'GRNL4', 'GSHP3', 'GUAR3', 'HAGA3', 'HAGA4', 'HAPV3', 'HBOR3', 'HBRE3', 'HBSA3',\n",
    "    'HBTS5', 'HETA3', 'HETA4', 'HGTX4', 'HOOT4', 'HRTP3', 'HYPE3', 'ICPI3', 'IENG3', 'IENG5', 'IFCM3', 'IGBR5', 'IGBR6', 'IGTI11', 'IGTI3', 'IGUA3', 'IGUA5', 'IGUA6', 'ILLS4', 'ILMD3',\n",
    "    'ILMD4', 'IMBI3', 'IMBI4', 'IMCH3', 'INEP3', 'INEP4', 'INET3', 'INHA3', 'INTB3', 'IRBR3', 'ISAE3', 'ISAE4', 'ITSA3', 'ITSA4', 'ITUB3', 'ITUB4', 'JALL3', 'JBSS3', 'JFAB4', 'JFEN3', \n",
    "    'JHSF3', 'JOPA3', 'JOPA4', 'JSLG3', 'KEPL3', 'KLBN11', 'KLBN3', 'KLBN4', 'KROT11', 'LAND3', 'LATM11', 'LATS3', 'LAVV3', 'LCSA3', 'LCSA4', 'LECO4', 'LETO3', 'LETO5', 'LEVE3', 'LEVE4',\n",
    "    'LFFE3', 'LFFE4', 'LGLO4', 'LIGH3', 'LIGT3', 'LIPR3', 'LIXC3', 'LIXC4', 'LJQQ3', 'LOGG3', 'LOGN3', 'LPSB3', 'LREN3', 'LREN4', 'LUPA3', 'LUXM3', 'LUXM4', 'LVTC3', 'LWSA3', 'MAGS3',\n",
    "    'MAPT3', 'MAPT4', 'MATD3', 'MBLY3', 'MDIA3', 'MDNE3', 'MEAL3', 'MELK3', 'MERC3', 'MERC4', 'MGEL3', 'MGEL4', 'MGLU3', 'MILS3', 'MLAS3', 'MLFT3', 'MLFT4', 'MLPA12', 'MLPA4', 'MMAQ3',\n",
    "    'MMAQ4', 'MNDL3', 'MNDL4', 'MNPR3', 'MNPR4', 'MNSA3', 'MNSA4', 'MOAR3', 'MOTV3', 'MOVI3', 'MRFG3', 'MRSA3B', 'MRSL3', 'MRSL4', 'MRVE3', 'MSAN3', 'MSAN4', 'MSPA3', 'MSPA4', 'MTBR3',\n",
    "    'MTBR4', 'MTIG3', 'MTRE3', 'MTSA4', 'MULT3', 'MWET3', 'MWET4', 'MYPK3', 'MYPK4', 'NATU3', 'NEOE3', 'NETC4', 'NEXP3', 'NGRD3', 'NORD3', 'NTCO3', 'NUTR3', 'ODER4', 'ODPV3', 'OFSA3',\n",
    "    'OGXP3', 'OIBR3', 'OIBR4', 'ONCO3', 'OPCT3', 'ORVR3', 'OSAO4', 'OSXB3', 'PALF11', 'PALF3', 'PALF5', 'PARC3', 'PATI3', 'PATI4', 'PCAR3', 'PDGR3', 'PDTC3', 'PEAB3', 'PEAB4', 'PEFX3',\n",
    "    'PEFX5', 'PETR3', 'PETR4', 'PETZ3', 'PFRM3', 'PGMN3', 'PINE3', 'PINE4', 'PITI4', 'PLAS3', 'PLDN4', 'PLIM4', 'PLPL3', 'PLTO5', 'PLTO6', 'PMAM3', 'PMAM4', 'PNOR5', 'PNOR6', 'PNVL3',\n",
    "    'POMO3', 'POMO4', 'POPR4', 'PORP4', 'PORT3', 'POSI3', 'PQUN3', 'PQUN4', 'PRBC4', 'PRGA4', 'PRIO3', 'PRML3', 'PRNR3', 'PRTX3', 'PRVI3', 'PSSA3', 'PTBL3', 'PTBL4', 'PTIP3', 'PTIP4',\n",
    "    'PTNT3', 'PTNT4', 'PTPA3', 'PTPA4', 'PTQS4', 'QUAL3', 'RADL3', 'RAIA3', 'RAIL3', 'RAIZ4', 'RANI3', 'RAPT3', 'RAPT4', 'RCSL3', 'RCSL4', 'RCTB31', 'RCTB33', 'RDCD3', 'RDNI3', 'RDOR3',\n",
    "    'RDTR3', 'REAG3', 'RECV3', 'REDE3', 'REDE4', 'REEM4', 'RENT3', 'REPA3', 'REPA4', 'RGEG3', 'RHDS3', 'RHDS4', 'RIPI3', 'RIPI4', 'RJCP3', 'RNAR3', 'RNEW11', 'RNEW3', 'RNEW4', 'RNPT3',\n",
    "    'RNPT4', 'ROMI3', 'ROMI4', 'RPAD3', 'RPAD5', 'RPAD6', 'RPMG3', 'RPMG4', 'RPSA4', 'RSID3', 'RSIP3', 'RSIP4', 'RSUL4', 'RUMO3', 'SALM3', 'SALM4', 'SANB11', 'SANB3', 'SANB4', 'SAPR11',\n",
    "    'SAPR3', 'SAPR4', 'SASG3', 'SBFG3', 'SBSP3', 'SCAR3', 'SCAR4', 'SCLO3', 'SCLO4', 'SDIA3', 'SDIA4', 'SEER3', 'SEMP3', 'SEQL3', 'SFSA4', 'SGAS3', 'SGAS4', 'SGEN3', 'SGEN4', 'SGPS3',\n",
    "    'SHOW3', 'SHUL4', 'SIMH3', 'SJOS3', 'SJOS4', 'SLCE3', 'SLCP3', 'SLED3', 'SLED4', 'SMFT3', 'SMLE3', 'SMTO3', 'SNSY5', 'SOJA3', 'SOND3', 'SOND5', 'SOND6', 'SRNA3', 'STBP11', 'STBP3',\n",
    "    'STRP4', 'SUBA3', 'SULT3', 'SULT4', 'SUZA4', 'SUZB3', 'SUZB5', 'SUZB6', 'SWET3', 'SYNE3', 'SZPQ4', 'TAEE11', 'TAEE3', 'TAEE4', 'TAMM3', 'TAMM4', 'TANC4', 'TASA3', 'TASA4', 'TBLE3',\n",
    "    'TBLE5', 'TBLE6', 'TCOC3', 'TCOC4', 'TCSA3', 'TCSL4', 'TDBH3', 'TDBH4', 'TECN3', 'TEFC11', 'TEKA3', 'TEKA4', 'TELB3', 'TELB4', 'TEMP3', 'TEND3', 'TENE5', 'TENE7', 'TERI3', 'TFCO4',\n",
    "    'TGMA3', 'TIBR3', 'TIBR5', 'TIBR6', 'TIMS3', 'TKNO4', 'TLCP3', 'TLCP4', 'TMAR3', 'TMAR5', 'TMAR6', 'TMCP3', 'TMCP4', 'TMGC11', 'TMGC12', 'TMGC13', 'TMGC3', 'TMGC7', 'TNCP3', 'TNCP4',\n",
    "    'TNEP3', 'TNEP4', 'TNLP3', 'TNLP4', 'TOTS3', 'TPIS3', 'TPRC3', 'TPRC6', 'TRAD3', 'TRFO3', 'TRFO4', 'TRIS3', 'TROR3', 'TROR4', 'TSEP3', 'TSEP4', 'TSPP3', 'TSPP4', 'TTEN3', 'TUPY3',\n",
    "    'TUPY4', 'TXRX3', 'TXRX4', 'UBBR11', 'UBBR3', 'UBBR4', 'UCAS3', 'UCOP4', 'UGPA3', 'UGPA4', 'UNIP3', 'UNIP5', 'UNIP6', 'USIM3', 'USIM5', 'USIM6', 'VAGR3', 'VAGV3', 'VAGV4', 'VALE3',\n",
    "    'VALE5', 'VAMO3', 'VBBR3', 'VCPA4', 'VGOR4', 'VIGR3', 'VINE3', 'VINE5', 'VITT3', 'VIVA3', 'VIVR3', 'VIVT3', 'VLID3', 'VPSC3', 'VPSC4', 'VPTA3', 'VPTA4', 'VSTE3',\n",
    "    'VTLM3', 'VTRU3', 'VULC3', 'VULC4', 'VVAR11', 'VVAR4', 'VVEO3', 'WEGE3', 'WEGE4', 'WEST3', 'WHRL3', 'WHRL4', 'WISA3', 'WISA4', 'WIZC3', 'WLMM3', 'WLMM4', 'WMBY3', 'YDUQ3', 'ZAMP3'\n",
    "    ]\n",
    "\n",
    "\n",
    "API_BASE = \"https://brapi.dev/api/quote\"\n",
    "TOKEN = \"TokenAPI\" \n",
    "\n",
    "BASE_DIR = os.path.abspath(\"DiretorioBaseDoProjeto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: summaryProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "FIELD_MAP = {\n",
    "    \"sector\": \"Setor\",\n",
    "    \"industry\": \"Subsetor\"\n",
    "}\n",
    "\n",
    "RATE_LIMIT_DELAY = 0.2\n",
    "\n",
    "def fetch_summary_profile(ticker: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Faz uma requisição à API para o ticker informado e retorna um dicionário com\n",
    "    os campos mapeados. Em caso de erro, retorna None e o ticker problemático.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"modules\": \"summaryProfile\",\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()[\"results\"][0]\n",
    "\n",
    "        # Alguns retornos não possuem a chave summaryProfile; usa nível superior como fallback\n",
    "        profile = data.get(\"summaryProfile\") or data\n",
    "\n",
    "        row = {\"Ticker\": ticker}\n",
    "        for field, name in FIELD_MAP.items():\n",
    "            # Converte None para string vazia para evitar erros na escrita CSV\n",
    "            value = profile.get(field)\n",
    "            row[name] = value if value is not None else \"\"\n",
    "        return row, None\n",
    "\n",
    "    except Exception as e:\n",
    "        # Retorna None e registra o ticker problemático\n",
    "        return None, (ticker, str(e))\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"Resultados\", exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "    tickers_problematic = []\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        print(f\" Buscando {ticker}...\")\n",
    "        row, error = fetch_summary_profile(ticker)\n",
    "        if row:\n",
    "            all_data.append(row)\n",
    "        if error:\n",
    "            print(f\" Erro ao buscar {error[0]}: {error[1]}\")\n",
    "            tickers_problematic.append(error[0])\n",
    "        time.sleep(RATE_LIMIT_DELAY) \n",
    "\n",
    "    if all_data:\n",
    "        output_path = Path(\"Resultados/summary_profile.csv\")\n",
    "        fieldnames = [\"Ticker\"] + list(FIELD_MAP.values())\n",
    "        with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_data)\n",
    "        print(f\"Dados exportados para {output_path.resolve()}\")\n",
    "\n",
    "    if tickers_problematic:\n",
    "        problematic_output = Path(\"Resultados/tickers_problematic.csv\")\n",
    "        with open(problematic_output, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Ticker\"])\n",
    "            for tk in tickers_problematic:\n",
    "                writer.writerow([tk])\n",
    "        print(f\" Tickers problemáticos exportados para {problematic_output.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: balanceSheetHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "MODULE = \"balanceSheetHistoryQuarterly\"\n",
    "\n",
    "FIELD_MAP_PT = {\n",
    "    \"endDate\": \"Data_Referencia\",\n",
    "    \"longName\": \"Nome_Empresa\",\n",
    "    \"cash\": \"Caixa\",\n",
    "    \"shortTermInvestments\": \"Investimentos_Curto_Prazo\",\n",
    "    \"netReceivables\": \"Recebiveis_Liquidos\",\n",
    "    \"inventory\": \"Estoque\",\n",
    "    \"otherCurrentAssets\": \"Outros_Ativos_Correntes\",\n",
    "    \"totalCurrentAssets\": \"Ativo_Circulante\",\n",
    "    \"longTermInvestments\": \"Investimentos_Long_Prazo\",\n",
    "    \"propertyPlantEquipment\": \"Imobilizado\",\n",
    "    \"otherAssets\": \"Outros_Ativos\",\n",
    "    \"totalAssets\": \"Ativo_Total\",\n",
    "    \"accountsPayable\": \"Contas_a_Pagar\",\n",
    "    \"shortLongTermDebt\": \"Divida_Curto_Long_Prazo\",\n",
    "    \"otherCurrentLiab\": \"Outros_Passivos_Correntes\",\n",
    "    \"longTermDebt\": \"Divida_Long_Prazo\",\n",
    "    \"otherLiab\": \"Outros_Passivos\",\n",
    "    \"totalCurrentLiabilities\": \"Passivo_Circulante\",\n",
    "    \"totalLiab\": \"Passivo_Total\",\n",
    "    \"commonStock\": \"Capital_Social\",\n",
    "    \"retainedEarnings\": \"Lucros_Acumulados\",\n",
    "    \"treasuryStock\": \"Acoes_Tesouraria\",\n",
    "    \"otherStockholderEquity\": \"Outros_Patrimonio_Liquido\",\n",
    "    \"totalStockholderEquity\": \"Patrimonio_Liquido\",\n",
    "    \"netTangibleAssets\": \"Ativos_Tangiveis_Liquidos\",\n",
    "    \"goodWill\": \"Goodwill\",\n",
    "    \"intangibleAssets\": \"Ativos_Intangiveis\",\n",
    "    \"deferredLongTermAssetCharges\": \"Encargos_Ativos_Long_Prazo\",\n",
    "    \"deferredLongTermLiab\": \"Passivo_Diferido_Long_Prazo\",\n",
    "    \"minorityInterest\": \"Interesse_Minoritario\",\n",
    "    \"capitalSurplus\": \"Excedente_Capital\",\n",
    "    \"otherLiabilities\": \"Outros_Passivos\",\n",
    "    \"accountsReceivableFromClients\": \"Contas_a_Receber_Clientes\",\n",
    "    \"accumulatedProfitsOrLosses\": \"Lucros_Prejuizos_Acumulados\",\n",
    "    \"capitalReserves\": \"Reservas_Capital\",\n",
    "    \"centralBankCompulsoryDeposit\": \"Deposito_Compulsorio_BC\",\n",
    "    \"controllerShareholdersEquity\": \"PL_Controlador\",\n",
    "    \"creditsWithRelatedParties\": \"Creditos_Partes_Relacionadas\",\n",
    "    \"cumulativeConversionAdjustments\": \"Ajustes_Conversao_Acumulada\",\n",
    "    \"currentAndDeferredTaxes\": \"Tributos_Correntes_Diferidos\",\n",
    "    \"currentLiabilities\": \"Passivo_Corrente\",\n",
    "    \"debentures\": \"Debentures\",\n",
    "    \"equityValuationAdjustments\": \"Ajustes_Avaliacao_Patrimonial\",\n",
    "    \"financialAssets\": \"Ativos_Financeiros\",\n",
    "    \"financialAssetsAtAmortizedCost\": \"Ativos_Financeiros_Custo_Amortizado\",\n",
    "    \"financialAssetsMeasuredAtFairValueThroughOtherComprehensiveIncome\": \"Ativos_Valor_Justo_Outros_Resultados\",\n",
    "    \"financialAssetsMeasuredAtFairValueThroughProfitOrLoss\": \"Ativos_Valor_Justo_Resultado\",\n",
    "    \"financialLiabilitiesAtAmortizedCost\": \"Passivos_Financeiros_Custo_Amortizado\",\n",
    "    \"financialLiabilitiesMeasuredAtFairValueThroughIncome\": \"Passivos_Valor_Justo_Resultado\",\n",
    "    \"foreignSuppliers\": \"Fornecedores_Estrangeiros\",\n",
    "    \"investments\": \"Investimentos\",\n",
    "    \"leaseFinancing\": \"Financiamento_Leasing\",\n",
    "    \"loansAndFinancing\": \"Emprestimos_Financiamentos\",\n",
    "    \"loansAndFinancingInForeignCurrency\": \"Emprestimos_Moeda_Estrangeira\",\n",
    "    \"loansAndFinancingInNationalCurrency\": \"Emprestimos_Moeda_Nacional\",\n",
    "    \"longTermAssets\": \"Ativos_Long_Prazo\",\n",
    "    \"longTermBiologicalAssets\": \"Ativos_Biologicos_Long_Prazo\",\n",
    "    \"longTermDebentures\": \"Debentures_Long_Prazo\",\n",
    "    \"longTermDeferredTaxes\": \"Tributos_Diferidos_Long_Prazo\",\n",
    "    \"longTermLeaseFinancing\": \"Financiamento_Leasing_Long_Prazo\",\n",
    "    \"longTermLoansAndFinancing\": \"Emprestimos_Financiamentos_Long_Prazo\",\n",
    "    \"longTermLoansAndFinancingInForeignCurrency\": \"Emprestimos_Estrangeira_Long_Prazo\",\n",
    "    \"longTermLoansAndFinancingInNationalCurrency\": \"Emprestimos_Nacional_Long_Prazo\",\n",
    "    \"longTermPrepaidExpenses\": \"Despesas_Anticipadas_Long_Prazo\",\n",
    "    \"longTermProvisions\": \"Provisoes_Long_Prazo\",\n",
    "    \"longTermRealizableAssets\": \"Ativos_Realizaveis_Long_Prazo\",\n",
    "    \"longTermReceivables\": \"Recebiveis_Long_Prazo\",\n",
    "    \"nationalSuppliers\": \"Fornecedores_Nacionais\",\n",
    "    \"nonControllingShareholdersEquity\": \"PL_Acionistas_Nao_Controladores\",\n",
    "    \"nonCurrentAssets\": \"Ativos_Nao_Correntes\",\n",
    "    \"nonCurrentLiabilities\": \"Passivo_Nao_Corrente\",\n",
    "    \"otherAccountsReceivable\": \"Outras_Contas_Receber\",\n",
    "    \"otherComprehensiveResults\": \"Outros_Resultados_Abrangentes\",\n",
    "    \"otherCurrentLiabilities\": \"Outros_Passivos_Correntes\",\n",
    "    \"otherLongTermObligations\": \"Outras_Obrigacoes_Long_Prazo\",\n",
    "    \"otherNonCurrentAssets\": \"Outros_Ativos_Nao_Correntes\",\n",
    "    \"otherObligations\": \"Outras_Obrigacoes\",\n",
    "    \"prepaidExpenses\": \"Despesas_Anticipadas\",\n",
    "    \"profitReserves\": \"Reservas_Lucros\",\n",
    "    \"providers\": \"Fornecedores\",\n",
    "    \"provisions\": \"Provisoes\",\n",
    "    \"realizedShareCapital\": \"Capital_Social_Realizado\",\n",
    "    \"revaluationReserves\": \"Reservas_Reavaliacao\",\n",
    "    \"shareholdersEquity\": \"Patrimonio_Liquido_Total\",\n",
    "    \"shareholdings\": \"Participacoes_Societarias\",\n",
    "    \"socialAndLaborObligations\": \"Obrigacoes_Sociais_Trabalhistas\",\n",
    "    \"taxLiabilities\": \"Obrigacoes_Tributarias\",\n",
    "    \"taxObligations\": \"Obrigacoes_Tributarias\",\n",
    "    \"taxesToRecover\": \"Tributos_a_Recuperar\",\n",
    "    \"type\": \"Tipo_Demonstrativo\",\n",
    "    \"updatedAt\": \"Atualizado_Em\",\n",
    "    \"biologicalAssets\": \"Ativos_Biologicos\",\n",
    "    \"deferredSellingExpenses\": \"Despesas_Vendas_Diferidas\",\n",
    "    \"deferredTaxes\": \"Tributos_Diferidos\",\n",
    "    \"financialInvestmentsMeasuredAtAmortizedCost\": \"Investimentos_Financeiros_Custo_Amortizado\",\n",
    "    \"financialInvestmentsMeasuredAtFairValueThroughOtherComprehensiveIncome\": \"Investimentos_Financeiros_Valor_Justo_Outros_Resultados\",\n",
    "    \"intangibleAsset\": \"Ativo_Intangivel\",\n",
    "    \"investmentProperties\": \"Propriedades_Investimento\",\n",
    "    \"longTermAccountsPayable\": \"Contas_Pagar_Long_Prazo\",\n",
    "    \"longTermAccountsReceivableFromClients\": \"Contas_Receber_Clientes_Long_Prazo\",\n",
    "    \"longTermFinancialInvestmentsMeasuredAtFairValueThroughIncome\": \"Investimentos_Financeiros_Long_Valor_Justo_Resultado\",\n",
    "    \"longTermInventory\": \"Estoque_Long_Prazo\",\n",
    "    \"longTermLiabilities\": \"Passivos_Long_Prazo\",\n",
    "    \"otherDebits\": \"Outros_Debitos\",\n",
    "    \"otherLongTermReceivables\": \"Outros_Recebiveis_Long_Prazo\",\n",
    "    \"otherNonCurrentLiabilities\": \"Outros_Passivos_Nao_Correntes\",\n",
    "    \"otherValuesAndAssets\": \"Outros_Valores_e_Ativos\",\n",
    "    \"profitsAndRevenuesToBeAppropriated\": \"Lucros_Receitas_A_Appropriar\",\n",
    "    \"securitiesAndCreditsReceivable\": \"Titulos_e_Creditos_a_Receber\"\n",
    "}\n",
    "\n",
    "# Tratamento de colunas redundantes\n",
    "REDUNDANT_GROUPS = {\n",
    "    \"Lucros_Acumulados_Completo\": [\"Lucros_Prejuizos_Acumulados\", \"Lucros_Acumulados\"],\n",
    "    \"Capital_Social_Completo\": [\"Capital_Social_Realizado\", \"Capital_Social\"],\n",
    "    \"Ativo_Total_Completo\": [\"Ativo_Total\", \"Ativos_Tangiveis_Liquidos\"],\n",
    "    \"Patrimonio_Liquido_Completo\": [\"Patrimonio_Liquido_Total\", \"Patrimonio_Liquido\"],\n",
    "    \"Divida_Bruta\": [\"Divida_Long_Prazo\", \"Divida_Curto_Long_Prazo\"],\n",
    "    \"Fornecedores_Completo\": [\"Fornecedores\", \"Contas_a_Pagar\"],\n",
    "    \"Outros_Passivos_Completo\": [\n",
    "        \"Outros_Passivos\", \"Outros_Passivos_Correntes\", \"Outros_Passivos_Nao_Correntes\",\n",
    "        \"otherLiab\", \"otherCurrentLiab\", \"otherLiabilities\", \"otherNonCurrentLiabilities\"\n",
    "    ],\n",
    "    \"Caixa_e_Investimentos_Completo\": [\n",
    "        \"Caixa_e_Investimentos_Curto_Prazo\", \"Caixa\", \"Investimentos_Curto_Prazo\"\n",
    "    ],\n",
    "    \"Tributos_Obrigacoes_Completo\": [\n",
    "        \"Obrigacoes_Tributarias\", \"Tributos_Correntes_Diferidos\", \"Tributos_Diferidos\"\n",
    "    ],\n",
    "    \"Reservas_Completo\": [\"Reservas_Capital\", \"Reservas_Lucros\", \"Reservas_Reavaliacao\"],\n",
    "        \"Passivo_Circulante_Completo\": [\"Passivo_Circulante\", \"Passivo_Corrente\"],\n",
    "    \"Passivo_Nao_Circulante_Completo\": [\"Passivo_Nao_Corrente\", \"Passivos_Long_Prazo\"]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "UNMAPPED_FIELDS = set()\n",
    "\n",
    "\n",
    "def fetch_data(ticker):\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    params = {\"modules\": MODULE, \"token\": TOKEN}\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data['results'][0]\n",
    "\n",
    "def _to_num(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_balance_sheet(data, ticker):\n",
    "    records = []\n",
    "\n",
    "    # 1) capturar 'longName' no topo do JSON (uma vez)\n",
    "    long_name = data.get(\"longName\")\n",
    "    print(f\" Processando {ticker} - Nome: {long_name if long_name else 'N/A'}\")\n",
    "    # se existir mapeamento pt para longName, usa; senão deixa 'longName'\n",
    "    long_name_key = FIELD_MAP_PT.get(\"longName\", \"longName\")\n",
    "\n",
    "    # 2) iterar sobre as entradas do módulo (ex.: 'balanceSheetHistoryQuarterly')\n",
    "    for entry in data.get(MODULE, []):\n",
    "        # base do registro, sempre com Ticker; inclui longName se existir\n",
    "        record = {\"Ticker\": ticker}\n",
    "        if long_name is not None:\n",
    "            record[long_name_key] = long_name\n",
    "\n",
    "        # 3) capturas para correções condicionais\n",
    "        total_assets = entry.get(\"totalAssets\")\n",
    "        total_liab = entry.get(\"totalLiab\")\n",
    "        shareholders_equity = entry.get(\"shareholdersEquity\") or entry.get(\"totalStockholderEquity\")\n",
    "\n",
    "        # 4) correção: se totalLiab veio igual a totalAssets (provável \"Passivo+PL\"), ajusta para só Passivo\n",
    "        assets = _to_num(entry.get(\"totalAssets\"))\n",
    "        liab   = _to_num(entry.get(\"totalLiab\"))\n",
    "        equity = _to_num(entry.get(\"shareholdersEquity\") or entry.get(\"totalStockholderEquity\"))\n",
    "\n",
    "        if None not in (assets, liab, equity):\n",
    "            tol = max(1.0, 1e-6 * abs(assets))  # tolerância p/ arredondamento\n",
    "            if abs(liab - assets) <= tol:\n",
    "                liab_corr = assets - equity\n",
    "                if liab_corr >= 0:  # sanidade mínima\n",
    "                    entry[\"totalLiab\"] = liab_corr\n",
    "                    print(f\" Corrigido totalLiab para {ticker}: {liab} → {liab_corr}\")\n",
    "\n",
    "        # 5) fallback de retainedEarnings\n",
    "        if entry.get(\"retainedEarnings\") in (None, 0):\n",
    "            fallback = entry.get(\"profitReserves\")\n",
    "            if fallback not in (None, 0):\n",
    "                entry[\"retainedEarnings\"] = fallback\n",
    "\n",
    "        # 6) mapeamento de todos os campos do entry\n",
    "        for k, v in entry.items():\n",
    "            key_pt = FIELD_MAP_PT.get(k)\n",
    "            if key_pt:\n",
    "                record[key_pt] = v\n",
    "            else:\n",
    "                if k not in UNMAPPED_FIELDS:\n",
    "                    UNMAPPED_FIELDS.add(k)\n",
    "                    print(f\"Campo não mapeado encontrado: {k}\")\n",
    "\n",
    "        # 7) adiciona o registro deste trimestre\n",
    "        records.append(record)\n",
    "\n",
    "    # 8) retorna lista de registros (um por trimestre)\n",
    "    return records\n",
    "\n",
    "\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "\n",
    "def save_csv(all_records):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar para CSV.\")\n",
    "        return\n",
    "\n",
    "    # 1. Salvar base completa\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV salvo: {MODULE}.csv\")\n",
    "\n",
    "    # 2. Salvar base enxuta\n",
    "    colunas_relevantes = [\n",
    "        \"Ticker\", \"Data_Referencia\", \"Tipo_Demonstrativo\",\n",
    "        \"Ativo_Circulante\", \"Ativo_Total_Completo\", \"Ativo_Total_Calculado\",\n",
    "        \"Caixa_e_Investimentos_Completo\", \"Recebiveis_Liquidos\", \"Estoque\",\n",
    "        \"Passivo_Circulante_Completo\", \"Passivo_Total\", \"Passivo_Total_Calculado\",\n",
    "        \"Divida_Bruta\", \"Emprestimos_Financiamentos\", \"Debentures\",\n",
    "        \"Patrimonio_Liquido_Completo\", \"Capital_Social_Completo\", \"Reservas_Completo\",\n",
    "        \"Lucros_Acumulados_Completo\", \"Imobilizado\"\n",
    "    ]\n",
    "\n",
    "    enxuto = []\n",
    "    for r in all_records:\n",
    "        enxuto.append({k: r.get(k, \"\") for k in colunas_relevantes if k in r})\n",
    "\n",
    "    if enxuto:\n",
    "        with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=colunas_relevantes)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(enxuto)\n",
    "        print(f\"CSV enxuto salvo: {MODULE}_enxuto.csv\")\n",
    "\n",
    "\n",
    "def calcular_campos_automaticos(record):\n",
    "    if \"Ativo_Circulante\" in record and \"Ativos_Nao_Correntes\" in record:\n",
    "        if record[\"Ativo_Circulante\"] not in (None, \"\", 0) and record[\"Ativos_Nao_Correntes\"] not in (None, \"\", 0):\n",
    "            record[\"Ativo_Total_Calculado\"] = record[\"Ativo_Circulante\"] + record[\"Ativos_Nao_Correntes\"]\n",
    "            print(\"Ativo_Total_Calculado gerado via soma de circulante + não circulante\")\n",
    "\n",
    "    if \"Passivo_Circulante\" in record and \"Passivo_Nao_Corrente\" in record:\n",
    "        if record[\"Passivo_Circulante\"] not in (None, \"\", 0) and record[\"Passivo_Nao_Corrente\"] not in (None, \"\", 0):\n",
    "            record[\"Passivo_Total_Calculado\"] = record[\"Passivo_Circulante\"] + record[\"Passivo_Nao_Corrente\"]\n",
    "            print(\"Passivo_Total_Calculado gerado via soma de circulante + não circulante\")\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def remover_campos_redundantes(record):\n",
    "    for final_col, alternatives in REDUNDANT_GROUPS.items():\n",
    "        for alt in alternatives:\n",
    "            if alt in record and record[alt] is not None and record[alt] != \"\":\n",
    "\n",
    "                record[final_col] = record[alt]\n",
    "                print(f\"Coluna '{final_col}' criada a partir de '{alt}'\")\n",
    "                break\n",
    "\n",
    "        for alt in alternatives:\n",
    "            if alt != final_col and alt in record:\n",
    "                del record[alt]\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_data = []\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" Coletando dados de {ticker}...\")\n",
    "            raw = fetch_data(ticker)\n",
    "            save_json(raw, ticker)\n",
    "            treated = parse_balance_sheet(raw, ticker)\n",
    "\n",
    "            for record in treated:\n",
    "                record = calcular_campos_automaticos(record)\n",
    "                record = remover_campos_redundantes(record)\n",
    "                all_data.append(record)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "            \n",
    "    # Lista de colunas a remover (completude <30% e não utilizadas para análise)\n",
    "    COLUNAS_DESCARTAVEIS = [\n",
    "    \"Acoes_Tesouraria\",\n",
    "    \"Ajustes_Conversao_Acumulada\",\n",
    "    \"Ativo_Intangivel\",\n",
    "    \"Ativos_Biologicos\",\n",
    "    \"Ativos_Financeiros\",\n",
    "    \"Ativos_Financeiros_Custo_Amortizado\",\n",
    "    \"Ativos_Valor_Justo_Outros_Resultados\",\n",
    "    \"Ativos_Valor_Justo_Resultado\",\n",
    "    \"Contas_Pagar_Long_Prazo\",\n",
    "    \"Contas_Receber_Clientes_Long_Prazo\",\n",
    "    \"Despesas_Anticipadas\",\n",
    "    \"Despesas_Anticipadas_Long_Prazo\",\n",
    "    \"Despesas_Vendas_Diferidas\",\n",
    "    \"Emprestimos_Estrangeira_Long_Prazo\",\n",
    "    \"Encargos_Ativos_Long_Prazo\",\n",
    "    \"Estoque_Long_Prazo\",\n",
    "    \"Excedente_Capital\",\n",
    "    \"Financiamento_Leasing\",\n",
    "    \"Financiamento_Leasing_Long_Prazo\",\n",
    "    \"Fornecedores_Estrangeiros\",\n",
    "    \"Goodwill\",\n",
    "    \"Investimentos_Financeiros_Custo_Amortizado\",\n",
    "    \"Investimentos_Financeiros_Long_Valor_Justo_Resultado\",\n",
    "    \"Investimentos_Financeiros_Valor_Justo_Outros_Resultados\",\n",
    "    \"Lucros_Receitas_A_Appropriar\",\n",
    "    \"Outros_Debitos\",\n",
    "    \"Outros_Recebiveis_Long_Prazo\",\n",
    "    \"Outros_Valores_e_Ativos\",\n",
    "    \"Passivos_Financeiros_Custo_Amortizado\",\n",
    "    \"Passivos_Long_Prazo\",\n",
    "    \"Recebiveis_Long_Prazo\",\n",
    "    \"Titulos_e_Creditos_a_Receber\"\n",
    "]\n",
    "\n",
    "    # Remover colunas irrelevantes ou com completude muito baixa\n",
    "    for record in all_data:\n",
    "        for col in COLUNAS_DESCARTAVEIS:\n",
    "            record.pop(col, None)\n",
    "\n",
    "    save_csv(all_data)\n",
    "\n",
    "    if UNMAPPED_FIELDS:\n",
    "        print(\"\\n Campos não mapeados:\")\n",
    "        for field in sorted(UNMAPPED_FIELDS):\n",
    "            print(\" -\", field)\n",
    "            \n",
    "    try:\n",
    "        df = pd.read_csv(f\"saida_tratada_quarterly/geral/{MODULE}.csv\")\n",
    "        nulls = df.isnull().mean().sort_values(ascending=False)\n",
    "        print(\"\\n Colunas com mais de 30% de valores nulos:\")\n",
    "        print(nulls[nulls > 0.3].apply(lambda x: f\"{x:.0%}\"))\n",
    "    except Exception as e:\n",
    "        print(f\" Não foi possível calcular a completude: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: defaultKeyStatisticsHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "MODULE = \"defaultKeyStatisticsHistoryQuarterly\"\n",
    "\n",
    "FIELD_MAP_PT = {\n",
    "    \"enterpriseValue\": \"Valor_Empresa\",\n",
    "    \"forwardPE\": \"Preco_Lucro_Projetado\",\n",
    "    \"profitMargins\": \"Margem_Liquida\",\n",
    "    \"sharesOutstanding\": \"Acoes_Emitidas\",\n",
    "    \"bookValue\": \"Valor_Patrimonial_Por_Acao\",\n",
    "    \"priceToBook\": \"Preco_PVP\",\n",
    "    \"mostRecentQuarter\": \"Data_Referencia\",\n",
    "    \"earningsQuarterlyGrowth\": \"Crescimento_Lucro_Trim\",\n",
    "    \"earningsAnnualGrowth\": \"Crescimento_Lucro_Anual\",\n",
    "    \"trailingEps\": \"Lucro_Por_Acao\",\n",
    "    \"enterpriseToRevenue\": \"EV_Receita\",\n",
    "    \"enterpriseToEbitda\": \"EV_EBITDA\",\n",
    "    \"dividendYield\": \"Dividend_Yield\",\n",
    "    \"lastDividendValue\": \"Ultimo_Dividendo\",\n",
    "    \"lastDividendDate\": \"Data_Ultimo_Dividendo\",\n",
    "    \"ytdReturn\": \"Retorno_Ano\",\n",
    "    \"52WeekChange\": \"Retorno_52_Semanas\",\n",
    "    \"totalAssets\": \"Total_Ativos\"\n",
    "}\n",
    "\n",
    "def fetch_key_statistics_quarterly(ticker):\n",
    "    params = {\n",
    "        \"modules\": MODULE,\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"results\"][0]\n",
    "    return result.get(MODULE, [])\n",
    "\n",
    "def parse_key_statistics_quarterly(data, ticker):\n",
    "    record = {\"Ticker\": ticker}\n",
    "    for k, v in data.items():\n",
    "        if k in FIELD_MAP_PT:\n",
    "            record[FIELD_MAP_PT[k]] = v\n",
    "        else:\n",
    "            record[k] = v\n",
    "\n",
    "\n",
    "    return record\n",
    "\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "def save_csv(all_records, module=\"defaultKeyStatisticsHistoryQuarterly\"):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"saida_tratada_quarterly\", exist_ok=True)\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV completo salvo em: saida_tratada_quarterly/{module}.csv\")\n",
    "\n",
    "    CAMPOS_PRIORITARIOS = [\n",
    "        \"Ticker\",\n",
    "        \"Data_Referencia\",\n",
    "        \"Preco_Lucro_Projetado\",\n",
    "        \"Margem_Liquida\",\n",
    "        \"Lucro_Por_Acao\",\n",
    "        \"Dividend_Yield\",\n",
    "        \"Preco_PVP\",\n",
    "        \"Valor_Patrimonial_Por_Acao\",\n",
    "        \"EV_EBITDA\",\n",
    "        \"Ultimo_Dividendo\",\n",
    "        \"Retorno_52_Semanas\"\n",
    "    ]\n",
    "    campos_filtrados = [c for c in CAMPOS_PRIORITARIOS if c in fieldnames]\n",
    "\n",
    "    with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=campos_filtrados)\n",
    "        writer.writeheader()\n",
    "        writer.writerows([{k: r.get(k, \"\") for k in campos_filtrados} for r in all_records])\n",
    "    print(f\"CSV enxuto salvo em: saida_tratada_quarterly/{module}_enxuto.csv\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    all_records = []\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" {ticker} - coletando dados...\")\n",
    "            data = fetch_key_statistics_quarterly(ticker)\n",
    "            save_json(data, ticker)\n",
    "\n",
    "            for entry in data:\n",
    "                parsed = parse_key_statistics_quarterly(entry, ticker)\n",
    "                all_records.append(parsed)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "    save_csv(all_records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: incomeStatementHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Configurações da API\n",
    "\n",
    "MODULE = \"incomeStatementHistoryQuarterly\"\n",
    "\n",
    "# Mapeamento de campos traduzidos\n",
    "FIELD_MAP_PT = {\n",
    "        \"endDate\": \"Data_Referencia\",\n",
    "        \"totalRevenue\": \"Receita_Liquida\",\n",
    "        \"costOfRevenue\": \"Custo_Receita\",\n",
    "        \"grossProfit\": \"Lucro_Bruto\",\n",
    "        \"sellingGeneralAdministrative\": \"Despesas_Vendas_Gerais\",\n",
    "        \"salesExpenses\": \"Despesas_Vendas\",\n",
    "        \"otherOperatingExpenses\": \"Outras_Despesas_Operacionais\",\n",
    "        \"operatingIncome\": \"Lucro_Operacional\",\n",
    "        \"incomeTaxExpense\": \"Imposto_Renda\",\n",
    "        \"netIncome\": \"Lucro_Liquido\",\n",
    "        \"ebit\": \"EBIT\",\n",
    "        \"interestExpense\": \"Despesas_Juros\",\n",
    "        \"totalOtherIncomeExpenseNet\": \"Outras_Receitas_Despesas_Liquidas\",\n",
    "        \"incomeBeforeTax\": \"Lucro_Antes_Impostos\",\n",
    "        \"financialResult\": \"Resultado_Financeiro\",\n",
    "        \"financialIncome\": \"Receita_Financeira\",\n",
    "        \"financialExpenses\": \"Despesa_Financeira\",\n",
    "        \"equityIncomeResult\": \"Resultado_Equivalencia_Patrimonial\",\n",
    "        \"basicEarningsPerCommonShare\": \"Lucro_Acao_Ordinaria\",\n",
    "        \"dilutedEarningsPerCommonShare\": \"Lucro_Acao_Ordinaria_Diluida\",\n",
    "    }\n",
    "\n",
    "# Função para buscar os dados da API\n",
    "def fetch_income_statement_quarterly(ticker):\n",
    "    params = {\n",
    "        \"modules\": MODULE,\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"results\"][0]\n",
    "    return result.get(MODULE, [])\n",
    "\n",
    "def parse_income_statement_quarterly(data, ticker):\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        record = {\"Ticker\": ticker}\n",
    "        for k, v in entry.items():\n",
    "            record[FIELD_MAP_PT.get(k, k)] = v\n",
    "\n",
    "        # Cálculo derivado: Despesas_Operacionais\n",
    "        despesas_raw = [\n",
    "            entry.get(\"sellingGeneralAdministrative\"),\n",
    "            entry.get(\"salesExpenses\"),\n",
    "            entry.get(\"otherOperatingExpenses\")\n",
    "        ]\n",
    "        despesas_operacionais = sum(v for v in despesas_raw if isinstance(v, (int, float)))\n",
    "        record[\"Despesas_Operacionais\"] = despesas_operacionais if despesas_operacionais != 0 else None\n",
    "\n",
    "        # Cálculo derivado: Margem_EBIT = EBIT / Receita Líquida\n",
    "        if \"EBIT\" in record and \"Receita_Liquida\" in record:\n",
    "            ebit = record[\"EBIT\"]\n",
    "            receita = record[\"Receita_Liquida\"]\n",
    "            if isinstance(ebit, (int, float)) and isinstance(receita, (int, float)) and receita != 0:\n",
    "                record[\"Margem_EBIT\"] = ebit / receita\n",
    "            else:\n",
    "                record[\"Margem_EBIT\"] = None\n",
    "\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "\n",
    "# Salvamento do JSON bruto\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "# Salvamento do CSV completo + enxuto\n",
    "def save_csv(all_records):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"saida_tratada_quarterly\", exist_ok=True)\n",
    "\n",
    "    # Obtem todos os campos presentes nos registros\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "\n",
    "    # Salva CSV completo\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV completo salvo em: saida_tratada_quarterly/{MODULE}.csv\")\n",
    "\n",
    "    # Campos essenciais para a versão enxuta\n",
    "    CAMPOS_PRIORITARIOS = [\n",
    "        \"Ticker\", \"Data_Referencia\", \"Receita_Total\", \"Lucro_Bruto\",\n",
    "        \"Despesas_Operacionais\", \"Lucro_Operacional\", \"Imposto_Renda\",\n",
    "        \"Lucro_Liquido\", \"EBIT\",\"Margem_EBIT\"\n",
    "    ]\n",
    "    campos_filtrados = [c for c in CAMPOS_PRIORITARIOS if c in fieldnames]\n",
    "\n",
    "    # Salva CSV enxuto\n",
    "    with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=campos_filtrados)\n",
    "        writer.writeheader()\n",
    "        for r in all_records:\n",
    "            row = {k: r.get(k, \"\") for k in campos_filtrados}\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV enxuto salvo em: saida_tratada_quarterly/{MODULE}_enxuto.csv\")\n",
    "\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "    \n",
    "    all_records = []\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" {ticker} - coletando dados...\")\n",
    "            data = fetch_income_statement_quarterly(ticker)\n",
    "            save_json(data, ticker)\n",
    "\n",
    "            parsed = parse_income_statement_quarterly(data, ticker)\n",
    "            all_records.extend(parsed)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "    save_csv(all_records)\n",
    "\n",
    "# Execução\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: financialDataHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações iniciais\n",
    "API_BASE = \"https://brapi.dev/api/quote\"\n",
    "TOKEN = \"nm8EdkfoD313BN6QP1aLyU\"\n",
    "MODULE = \"financialDataHistoryQuarterly\"\n",
    "\n",
    "# Mapemento de campos para nomes padronizados\n",
    "FIELD_MAP_PT = {\n",
    "    \"currentPrice\": \"Preco_Atual\",\n",
    "    \"currentRatio\": \"Liquidez_Corrente\",\n",
    "    \"quickRatio\": \"Liquidez_Seca\",\n",
    "    \"debtToEquity\": \"Divida_Patrimonio\",\n",
    "    \"earningsGrowth\": \"Crescimento_Lucro\",\n",
    "    \"ebitda\": \"EBITDA\",\n",
    "    \"ebitdaMargins\": \"Margem_EBITDA\",\n",
    "    \"financialCurrency\": \"Moeda_Financeira\",\n",
    "    \"freeCashflow\": \"Fluxo_Caixa_Livre\",\n",
    "    \"grossMargins\": \"Margem_Bruta\",\n",
    "    \"grossProfits\": \"Lucro_Bruto_Acum\",\n",
    "    \"operatingCashflow\": \"Fluxo_Caixa_Operacional\",\n",
    "    \"operatingMargins\": \"Margem_Operacional\",\n",
    "    \"profitMargins\": \"Margem_Liquida\",\n",
    "    \"returnOnAssets\": \"ROA\",\n",
    "    \"returnOnEquity\": \"ROE\",\n",
    "    \"revenueGrowth\": \"Crescimento_Receita\",\n",
    "    \"revenuePerShare\": \"Receita_Por_Acao\",\n",
    "    \"symbol\": \"Ticker\",\n",
    "    \"totalCash\": \"Caixa_Total\",\n",
    "    \"totalCashPerShare\": \"Caixa_Por_Acao\",\n",
    "    \"totalDebt\": \"Divida_Total\",\n",
    "    \"totalRevenue\": \"Receita_Total\",\n",
    "    \"updatedAt\": \"Data_Referencia\"\n",
    "}\n",
    "\n",
    "\n",
    "# Função para requisitar os dados da API\n",
    "def fetch_financial_data_quarterly(ticker):\n",
    "    params = {\n",
    "        \"modules\": MODULE,\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"results\"][0]\n",
    "    return result.get(MODULE, [])\n",
    "\n",
    "# Função de parsing\n",
    "\n",
    "def parse_financial_data_quarterly(data, ticker):\n",
    "    records = []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(f\" Estrutura inesperada para {ticker}: esperado list, recebido {type(data)}\")\n",
    "        return records\n",
    "\n",
    "    for entry in data:\n",
    "        if not isinstance(entry, dict):\n",
    "            print(f\" Entrada inesperada no array de {ticker}: {entry}\")\n",
    "            continue\n",
    "\n",
    "        record = {\"Ticker\": ticker}\n",
    "        for k, v in entry.items():\n",
    "            record[FIELD_MAP_PT.get(k, k)] = v\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# Função para salvar JSON bruto\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "# Função para salvar CSV completo e enxuto\n",
    "def save_csv(all_records):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"saida_tratada_quarterly\", exist_ok=True)\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "\n",
    "    # CSV completo\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV completo salvo em: saida_tratada_quarterly/{MODULE}.csv\")\n",
    "\n",
    "    # CSV enxuto\n",
    "    CAMPOS_PRIORITARIOS = [\n",
    "        \"Ticker\", \"Data_Referencia\", \"Fluxo_Caixa_Operacional\", \"Investimentos_Capex\",\n",
    "        \"Fluxo_Caixa_Livre\", \"Margem_Bruta\", \"Margem_EBITDA\", \"Margem_Operacional\",\n",
    "        \"Margem_Liquida\", \"ROA\", \"ROE\", \"Crescimento_Receita\", \"EBITDA\",\n",
    "        \"Lucro_Liquido_Acum\", \"Divida_Patrimonio\", \"Preco_Atual\", \"Receita_Por_Acao\"\n",
    "    ]\n",
    "    campos_filtrados = [c for c in CAMPOS_PRIORITARIOS if c in fieldnames]\n",
    "\n",
    "    with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=campos_filtrados)\n",
    "        writer.writeheader()\n",
    "        writer.writerows([{k: r.get(k, \"\") for k in campos_filtrados} for r in all_records])\n",
    "    print(f\"CSV enxuto salvo em: saida_tratada_quarterly/{MODULE}_enxuto.csv\")\n",
    "\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "   \n",
    "    \n",
    "    all_records = []\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" {ticker} - coletando dados...\")\n",
    "            data = fetch_financial_data_quarterly(ticker)\n",
    "            save_json(data, ticker)\n",
    "            parsed = parse_financial_data_quarterly(data, ticker)\n",
    "            all_records.extend(parsed)\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "    save_csv(all_records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo: valueAddedHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações iniciais\n",
    "API_BASE = \"https://brapi.dev/api/quote\"\n",
    "TOKEN = \"TOKEN\"\n",
    "MODULE = \"valueAddedHistoryQuarterly\"\n",
    "\n",
    "# Mapeamento de campos para nomes padronizados\n",
    "FIELD_MAP_PT = {\n",
    "    \"symbol\": \"Ticker\",\n",
    "    \"endDate\": \"Data_Referencia\",\n",
    "    \"revenue\": \"Receita_Total\",\n",
    "    \"financialIntermediationRevenue\": \"Receita_Intermediacao_Financeira\",\n",
    "    \"revenueFromTheProvisionOfServices\": \"Receita_Servicos\",\n",
    "    \"provisionOrReversalOfExpectedCreditRiskLosses\": \"Provisao_Reversao_Risco_Credito\",\n",
    "    \"otherRevenues\": \"Outras_Receitas\",\n",
    "    \"financialIntermediationExpenses\": \"Despesas_Intermediacao_Financeira\",\n",
    "    \"suppliesPurchasedFromThirdParties\": \"Suprimentos_Terceiros\",\n",
    "    \"materialsEnergyAndOthers\": \"Materiais_Energia_Outros\",\n",
    "    \"services\": \"Servicos_Terceiros\",\n",
    "    \"lossOrRecoveryOfAssetValues\": \"Perda_Recuperacao_Ativos\",\n",
    "    \"otherSupplies\": \"Outros_Suprimentos\",\n",
    "    \"grossAddedValue\": \"Valor_Adicionado_Bruto\",\n",
    "    \"retentions\": \"Retencoes\",\n",
    "    \"depreciationAndAmortization\": \"Depreciacao_Amortizacao\",\n",
    "    \"otherRetentions\": \"Outras_Retencoes\",\n",
    "    \"netAddedValue\": \"Valor_Adicionado_Liquido\",\n",
    "    \"addedValueReceivedByTransfer\": \"Valor_Adicionado_Recebido_Transferencia\",\n",
    "    \"equityIncomeResult\": \"Resultado_Equivalencia_Patrimonial\",\n",
    "    \"otherValuesReceivedByTransfer\": \"Outros_Valores_Recebidos_Transferencia\",\n",
    "    \"addedValueToDistribute\": \"Valor_Adicionado_Distribuir\",\n",
    "    \"distributionOfAddedValue\": \"Distribuicao_Valor_Adicionado\",\n",
    "    \"teamRemuneration\": \"Remuneracao_Equipe\",\n",
    "    \"taxes\": \"Tributos_Totais\",\n",
    "    \"federalTaxes\": \"Tributos_Federais\",\n",
    "    \"stateTaxes\": \"Tributos_Estaduais\",\n",
    "    \"municipalTaxes\": \"Tributos_Municipais\",\n",
    "    \"remunerationOfThirdPartyCapitals\": \"Remuneracao_Capital_Terceiros\",\n",
    "    \"equityRemuneration\": \"Remuneracao_Proprios\",\n",
    "    \"interestOnOwnEquity\": \"Juros_Sobre_Capital_Proprio\",\n",
    "    \"dividends\": \"Dividendos\",\n",
    "    \"retainedEarningsOrLoss\": \"Lucros_Prejuizos_Retidos\",\n",
    "    \"nonControllingShareOfRetainedEarnings\": \"Participacao_Minoritarios\",\n",
    "    \"otherDistributions\": \"Outras_Distribuicoes\",\n",
    "    \"productSales\": \"Vendas_Produtos\",\n",
    "    \"constructionOfOwnAssets\": \"Construcao_Propria\",\n",
    "    \"provisionOrReversalOfDoubtfulAccounts\": \"Provisao_Reversao_Duvidosos\",\n",
    "    \"costsWithProductsSold\": \"Custos_Produtos_Vendidos\",\n",
    "    \"thirdPartyMaterialsAndServices\": \"Materiais_Servicos_Terceiros\",\n",
    "    \"netAddedValueProduced\": \"Valor_Adicionado_Liquido_Produzido\",\n",
    "    \"addedValueReceivedOnTransfer\": \"Valor_Adicionado_Recebido_Transferencia_2\",\n",
    "    \"financialIncome\": \"Receita_Financeira\",\n",
    "    \"otherValuesReceivedOnTransfer\": \"Outros_Valores_Recebidos_Transferencia_2\",\n",
    "    \"ownEquityRemuneration\": \"Remuneracao_Capital_Proprio\",\n",
    "    \"insuranceOperationsRevenue\": \"Receita_Operacoes_Seguros\",\n",
    "    \"complementaryPensionOperationsRevenue\": \"Receita_Prev_Complementar\",\n",
    "    \"feesRevenue\": \"Receita_Taxas\",\n",
    "    \"variationsOfTechnicalProvisions\": \"Variacao_Provisoes_Tecnicas\",\n",
    "    \"insuranceOperationsVariations\": \"Variacao_Operacoes_Seguros\",\n",
    "    \"pensionOperationsVariations\": \"Variacao_Operacoes_Prev\",\n",
    "    \"otherVariations\": \"Outras_Variacoes\",\n",
    "    \"netOperatingRevenue\": \"Receita_Operacional_Liquida\",\n",
    "    \"claimsAndBenefits\": \"Sinistros_Beneficios\",\n",
    "    \"variationInDeferredSellingExpenses\": \"Variacao_Despesas_Vendas_Diferidas\",\n",
    "    \"resultsOfCededReinsuranceOperations\": \"Resultado_Reaseguro_Cedido\",\n",
    "    \"resultOfCoinsuranceOperationsAssigned\": \"Resultado_Co_Seguros_Assumidos\",\n",
    "    \"totalAddedValueToDistribute\": \"Valor_Adicionado_Total_Distribuir\",\n",
    "    \"updatedAt\": \"Atualizado_Em\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Função para requisitar os dados da API\n",
    "def fetch_value_added_quarterly(ticker):\n",
    "    params = {\n",
    "        \"modules\": MODULE,\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"results\"][0]\n",
    "    return result.get(MODULE, [])\n",
    "\n",
    "# Função de parsing\n",
    "def parse_value_added_quarterly(data, ticker):\n",
    "    records = []\n",
    "    if not isinstance(data, list):\n",
    "        print(f\" Estrutura inesperada para {ticker}: esperado list, recebido {type(data)}\")\n",
    "        return records\n",
    "\n",
    "    for entry in data:\n",
    "        if not isinstance(entry, dict):\n",
    "            print(f\" Entrada inesperada no array de {ticker}: {entry}\")\n",
    "            continue\n",
    "        record = {\"Ticker\": ticker}\n",
    "        for k, v in entry.items():\n",
    "            record[FIELD_MAP_PT.get(k, k)] = v\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "# Função para salvar JSON bruto\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "# Função para salvar CSV completo e enxuto\n",
    "def save_csv(all_records):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"saida_tratada_quarterly\", exist_ok=True)\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "\n",
    "    # CSV completo\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV completo salvo em: saida_tratada_quarterly/{MODULE}.csv\")\n",
    "\n",
    "    # CSV enxuto\n",
    "    CAMPOS_PRIORITARIOS = [\n",
    "    \"Ticker\", \"Data_Referencia\",\n",
    "    \"Receita_Total\", \"Vendas_Produtos\", \"Outras_Receitas\",\n",
    "    \"Suprimentos_Terceiros\", \"Custos_Produtos_Vendidos\", \"Materiais_Servicos_Terceiros\",\n",
    "    \"Valor_Adicionado_Bruto\", \"Depreciacao_Amortizacao\", \"Valor_Adicionado_Liquido\",\n",
    "    \"Resultado_Equivalencia_Patrimonial\", \"Valor_Adicionado_Distribuir\", \"Distribuicao_Valor_Adicionado\",\n",
    "    \"Remuneracao_Equipe\", \"Tributos_Totais\", \"Tributos_Federais\", \"Tributos_Estaduais\", \"Tributos_Municipais\",\n",
    "    \"Remuneracao_Capital_Terceiros\", \"Juros_Sobre_Capital_Proprio\", \"Dividendos\", \"Lucros_Prejuizos_Retidos\",\n",
    "    \"Participacao_Minoritarios\", \"Remuneracao_Proprios\", \"Valor_Adicionado_Recebido_Transferencia\",\n",
    "    \"Valor_Adicionado_Liquido_Produzido\", \"Receita_Intermediacao_Financeira\",\"Receita_Servicos\",\"Receita_Financeira\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    campos_filtrados = [c for c in CAMPOS_PRIORITARIOS if c in fieldnames]\n",
    "\n",
    "    with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=campos_filtrados)\n",
    "        writer.writeheader()\n",
    "        writer.writerows([{k: r.get(k, \"\") for k in campos_filtrados} for r in all_records])\n",
    "    print(f\"CSV enxuto salvo em: saida_tratada_quarterly/{MODULE}_enxuto.csv\")\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "    \n",
    "    all_records = []\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" {ticker} - coletando dados...\")\n",
    "            data = fetch_value_added_quarterly(ticker)\n",
    "            save_json(data, ticker)\n",
    "            parsed = parse_value_added_quarterly(data, ticker)\n",
    "            all_records.extend(parsed)\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "    save_csv(all_records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modulo: cashflowHistoryQuarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações iniciais\n",
    "API_BASE = \"https://brapi.dev/api/quote\"\n",
    "TOKEN = \"TOKEN\"\n",
    "MODULE = \"cashflowHistoryQuarterly\"\n",
    "\n",
    "# Mapeamento de campos da API para nomes padronizados em português\n",
    "FIELD_MAP_PT = {\n",
    "    \"symbol\": \"Ticker\",\n",
    "    \"endDate\": \"Data_Referencia\",\n",
    "    \"operatingCashFlow\": \"Fluxo_Caixa_Operacional\",\n",
    "    \"incomeFromOperations\": \"Lucro_Operacional\",\n",
    "    \"netIncomeBeforeTaxes\": \"Lucro_Antes_Impostos\",\n",
    "    \"adjustmentsToProfitOrLoss\": \"Ajustes_Lucro_Prejuizo\",\n",
    "    \"changesInAssetsAndLiabilities\": \"Variacao_Ativos_Passivos\",\n",
    "    \"otherOperatingActivities\": \"Outras_Atividades_Operacionais\",\n",
    "    \"investmentCashFlow\": \"Fluxo_Caixa_Investimentos\",\n",
    "    \"financingCashFlow\": \"Fluxo_Caixa_Financiamentos\",\n",
    "    \"exchangeVariationWithoutCash\": \"Variacao_Cambial_Sem_Efeito_Caixa\",\n",
    "    \"foreignExchangeRateWithoutCash\": \"Variacao_Cambial_Sem_Efeito_Caixa\",  # mapeamento alternativo\n",
    "    \"increaseOrDecreaseInCash\": \"Variacao_Liquida_Caixa\",\n",
    "    \"initialCashBalance\": \"Caixa_Inicial\",\n",
    "    \"finalCashBalance\": \"Caixa_Final\",\n",
    "    \"cashGeneratedInOperations\": \"Caixa_Gerado_Operacoes\",\n",
    "    \"updatedAt\": \"Atualizado_Em\"\n",
    "}\n",
    "\n",
    "# Função para requisitar os dados da API\n",
    "def fetch_cashflow_data_quarterly(ticker):\n",
    "    params = {\n",
    "        \"modules\": MODULE,\n",
    "        \"token\": TOKEN\n",
    "    }\n",
    "    url = f\"{API_BASE}/{ticker}\"\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"results\"][0]\n",
    "    return result.get(MODULE, [])\n",
    "\n",
    "# Função para parsear os dados\n",
    "def parse_cashflow_data_quarterly(data, ticker):\n",
    "    records = []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(f\" Estrutura inesperada para {ticker}: esperado list, recebido {type(data)}\")\n",
    "        return records\n",
    "\n",
    "    for entry in data:\n",
    "        if not isinstance(entry, dict):\n",
    "            print(f\" Entrada inesperada no array de {ticker}: {entry}\")\n",
    "            continue\n",
    "\n",
    "        record = {\"Ticker\": ticker}\n",
    "        for k, v in entry.items():\n",
    "            record[FIELD_MAP_PT.get(k, k)] = v\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "# Função para salvar o JSON bruto\n",
    "def save_json(data, ticker):\n",
    "    path = Path(f\"saida_bruta_quarterly/raw_{MODULE}_{ticker}.json\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"JSON salvo: {path}\")\n",
    "\n",
    "# Função para salvar o CSV completo e enxuto\n",
    "def save_csv(all_records):\n",
    "    if not all_records:\n",
    "        print(\" Nenhum dado para exportar.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"saida_tratada_quarterly\", exist_ok=True)\n",
    "    fieldnames = sorted(set().union(*(r.keys() for r in all_records)))\n",
    "\n",
    "    # CSV completo\n",
    "    with open(f\"saida_tratada_quarterly/geral/{MODULE}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_records)\n",
    "    print(f\"CSV completo salvo em: saida_tratada_quarterly/{MODULE}.csv\")\n",
    "\n",
    "    # CSV enxuto (campos prioritários)\n",
    "    CAMPOS_PRIORITARIOS = [\n",
    "        \"Ticker\",\n",
    "        \"Data_Referencia\",\n",
    "        \"Fluxo_Caixa_Operacional\",\n",
    "        \"Fluxo_Caixa_Investimentos\",\n",
    "        \"Fluxo_Caixa_Financiamentos\",\n",
    "        \"Variacao_Liquida_Caixa\",\n",
    "        \"Caixa_Final\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    campos_filtrados = [c for c in CAMPOS_PRIORITARIOS if c in fieldnames]\n",
    "\n",
    "    with open(f\"saida_tratada_quarterly/enxuto/{MODULE}_enxuto.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=campos_filtrados)\n",
    "        writer.writeheader()\n",
    "        writer.writerows([{k: r.get(k, \"\") for k in campos_filtrados} for r in all_records])\n",
    "    print(f\"CSV enxuto salvo em: saida_tratada_quarterly/{MODULE}_enxuto.csv\")\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "    \n",
    "    all_records = []\n",
    "    for ticker in TICKERS:\n",
    "        try:\n",
    "            print(f\" {ticker} - coletando dados...\")\n",
    "            data = fetch_cashflow_data_quarterly(ticker)\n",
    "            save_json(data, ticker)\n",
    "            parsed = parse_cashflow_data_quarterly(data, ticker)\n",
    "            all_records.extend(parsed)\n",
    "        except Exception as e:\n",
    "            print(f\" Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "    save_csv(all_records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Junção de todas as bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# ============================================================\n",
    "# Caminhos\n",
    "# ============================================================\n",
    "CAMINHO_BASES   = r\"Caminho\\saida_tratada_quarterly\\geral\"\n",
    "ARQUIVO_SUMMARY = \"summary_profile.csv\"\n",
    "\n",
    "# Arquivos de bases trimestrais\n",
    "arquivos = [\n",
    "    \"balanceSheetHistoryQuarterly.csv\",\n",
    "    \"cashflowHistoryQuarterly.csv\",\n",
    "    \"defaultKeyStatisticsHistoryQuarterly.csv\",\n",
    "    \"financialDataHistoryQuarterly.csv\",\n",
    "    \"incomeStatementHistoryQuarterly.csv\",\n",
    "    \"valueAddedHistoryQuarterly.csv\",\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1) Carregar cada base em um dicionário\n",
    "# ============================================================\n",
    "bases = {}\n",
    "for nome in arquivos:\n",
    "    caminho = os.path.join(CAMINHO_BASES, nome)\n",
    "    df = pd.read_csv(caminho)\n",
    "\n",
    "    if not {\"Ticker\", \"Data_Referencia\"}.issubset(df.columns):\n",
    "        raise ValueError(f\" Base '{nome}' não contém Ticker/Data_Referencia\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"Ticker\", \"Data_Referencia\"])\n",
    "    bases[nome.replace(\".csv\", \"\")] = df\n",
    "\n",
    "print(\"Bases carregadas\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Merge progressivo\n",
    "# ============================================================\n",
    "def merge_drop_dups(left, right):\n",
    "    merged = pd.merge(left, right,\n",
    "                      on=[\"Ticker\", \"Data_Referencia\"],\n",
    "                      how=\"outer\",\n",
    "                      suffixes=('', '_dup'))\n",
    "    return merged.drop(columns=[c for c in merged.columns if c.endswith('_dup')])\n",
    "\n",
    "df_merged = reduce(merge_drop_dups, list(bases.values()))\n",
    "\n",
    "# ============================================================\n",
    "# 3) Cálculos adicionais\n",
    "# ------------------------------------------------------------\n",
    "# 3a) Divida_Bruta (caso não exista)\n",
    "# ------------------------------------------------------------\n",
    "if \"Divida_Bruta\" not in df_merged.columns:\n",
    "    df_merged[\"Divida_Bruta\"] = df_merged[[\"shortLongTermDebt\",\n",
    "                                           \"longTermDebt\"]].sum(axis=1, skipna=True)\n",
    "\n",
    "# 3b) Patrimônio Líquido (alias)\n",
    "if \"totalStockholderEquity\" in df_merged.columns and \\\n",
    "   \"Patrimonio_Liquido_Completo\" not in df_merged.columns:\n",
    "    df_merged[\"Patrimonio_Liquido_Completo\"] = df_merged[\"totalStockholderEquity\"]\n",
    "\n",
    "# 3c) Divida_Bruta_PL\n",
    "df_merged[\"Divida_Bruta_PL\"] = df_merged.apply(\n",
    "    lambda r: r[\"Divida_Bruta\"] / r[\"Patrimonio_Liquido_Completo\"]\n",
    "              if pd.notnull(r[\"Divida_Bruta\"])\n",
    "              and pd.notnull(r[\"Patrimonio_Liquido_Completo\"])\n",
    "              and r[\"Patrimonio_Liquido_Completo\"] != 0 else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3d) Preço da ação – detectar coluna\n",
    "# ------------------------------------------------------------\n",
    "POSSIVEIS_PRECOS = [\"currentPrice\", \"regularMarketPrice\", \"Preco_Atual\"]\n",
    "col_preco = next((c for c in POSSIVEIS_PRECOS if c in df_merged.columns), None)\n",
    "\n",
    "# Renomear para \"Preco_Atual\" se existir e ainda não houver\n",
    "if col_preco and col_preco != \"Preco_Atual\":\n",
    "    df_merged.rename(columns={col_preco: \"Preco_Atual\"}, inplace=True)\n",
    "    col_preco = \"Preco_Atual\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3e) Cálculo do P/L (Preco_Lucro)\n",
    "# ------------------------------------------------------------\n",
    "if \"Preco_Lucro\" not in df_merged.columns:\n",
    "    df_merged[\"Preco_Lucro\"] = None  # cria coluna vazia\n",
    "\n",
    "if col_preco and \"Lucro_Por_Acao\" in df_merged.columns:\n",
    "    mask = df_merged[\"Preco_Lucro\"].isna() & \\\n",
    "           df_merged[col_preco].notna() & \\\n",
    "           df_merged[\"Lucro_Por_Acao\"].notna() & \\\n",
    "           (df_merged[\"Lucro_Por_Acao\"] != 0)\n",
    "\n",
    "    df_merged.loc[mask, \"Preco_Lucro\"] = (\n",
    "        df_merged.loc[mask, col_preco] / df_merged.loc[mask, \"Lucro_Por_Acao\"]\n",
    "    )\n",
    "\n",
    "    print(f\"P/L calculado para {mask.sum()} linhas usando coluna de preço '{col_preco}'\")\n",
    "else:\n",
    "    print(\" Não foi possível calcular P/L – coluna de preço ou Lucro_Por_Acao ausente.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Adiciona setor\n",
    "# ============================================================\n",
    "df_summary = pd.read_csv(ARQUIVO_SUMMARY)\n",
    "if \"Setor\" in df_summary.columns:\n",
    "    df_summary = df_summary.rename(columns={\"Setor\": \"Setor_Oficial\"})\n",
    "\n",
    "df_final = pd.merge(df_merged,\n",
    "                    df_summary[[\"Ticker\", \"Setor_Oficial\"]],\n",
    "                    on=\"Ticker\", how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Ordenação de colunas\n",
    "# ============================================================\n",
    "primeiras = ['Ticker', 'Setor_Oficial', 'Data_Referencia']\n",
    "restantes = [c for c in df_final.columns if c not in primeiras]\n",
    "df_final = df_final[primeiras + restantes]\n",
    "\n",
    "# ============================================================\n",
    "# 6) Salvar CSV final\n",
    "# ============================================================\n",
    "df_final.to_csv(\"base_unificada_quarterly.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Base final salva como 'base_unificada_quarterly.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para a pasta contendo os arquivos .json\n",
    "PASTA_JSON = \"saida_bruta_quarterly\"\n",
    "colunas_por_modulo = {}\n",
    "\n",
    "# Função para extrair as chaves de um JSON, mesmo se for uma lista\n",
    "def extrair_chaves(obj):\n",
    "    if isinstance(obj, list):\n",
    "        chaves = set()\n",
    "        for item in obj:\n",
    "            chaves.update(item.keys())\n",
    "        return chaves\n",
    "    elif isinstance(obj, dict):\n",
    "        return set(obj.keys())\n",
    "    return set()\n",
    "\n",
    "# Iterar por todos os arquivos JSON\n",
    "for arquivo in os.listdir(PASTA_JSON):\n",
    "    if not arquivo.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    modulo = arquivo.replace(\"raw_\", \"\").split(\"_\")[0]  # Ex: balanceSheetHistoryQuarterly\n",
    "    caminho_arquivo = os.path.join(PASTA_JSON, arquivo)\n",
    "\n",
    "    try:\n",
    "        with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
    "            dados = json.load(f)\n",
    "\n",
    "        # Verifica se há apenas um módulo na raiz (como é padrão na brapi)\n",
    "        if isinstance(dados, dict) and modulo in dados:\n",
    "            dados_modulo = dados[modulo]\n",
    "        else:\n",
    "            dados_modulo = dados  # fallback se for uma lista ou não estiver aninhado\n",
    "\n",
    "        chaves = extrair_chaves(dados_modulo)\n",
    "\n",
    "        # Adiciona as chaves ao dicionário do módulo\n",
    "        if modulo not in colunas_por_modulo:\n",
    "            colunas_por_modulo[modulo] = set()\n",
    "        colunas_por_modulo[modulo].update(chaves)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao ler {arquivo}: {e}\")\n",
    "\n",
    "# Criar DataFrame consolidado\n",
    "linhas = []\n",
    "for modulo, colunas in colunas_por_modulo.items():\n",
    "    for coluna in sorted(colunas):\n",
    "        linhas.append({\"Módulo\": modulo, \"Coluna\": coluna})\n",
    "\n",
    "df_resultado = pd.DataFrame(linhas)\n",
    "\n",
    "# Salvar em CSV\n",
    "df_resultado.to_csv(\"colunas_por_modulo.csv\", index=False)\n",
    "print(\"Colunas extraídas com sucesso e salvas em 'colunas_por_modulo.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_Debt: 0        3.638700e+08\n",
      "1        3.213530e+08\n",
      "2        3.263030e+08\n",
      "3        3.170240e+08\n",
      "4        1.434840e+08\n",
      "             ...     \n",
      "22966    1.083107e+09\n",
      "22967    1.087355e+09\n",
      "22968    1.009701e+09\n",
      "22969    1.030205e+09\n",
      "22970    7.913740e+08\n",
      "Name: Net_Debt, Length: 22971, dtype: float64\n",
      "EBIT: 0                NaN\n",
      "1        131399000.0\n",
      "2        132955000.0\n",
      "3        139940000.0\n",
      "4        100612000.0\n",
      "            ...     \n",
      "22966     90489000.0\n",
      "22967     84544000.0\n",
      "22968    -35632000.0\n",
      "22969     -1567000.0\n",
      "22970     -7743000.0\n",
      "Name: EBIT, Length: 22971, dtype: float64\n",
      "EV: 0        3.638700e+08\n",
      "1        3.213530e+08\n",
      "2        3.263030e+08\n",
      "3        3.170240e+08\n",
      "4        1.477249e+09\n",
      "             ...     \n",
      "22966    1.029829e+09\n",
      "22967    7.131706e+08\n",
      "22968    9.196717e+08\n",
      "22969    1.204526e+09\n",
      "22970    1.387646e+09\n",
      "Name: Valor_Empresa, Length: 22971, dtype: float64\n",
      "EV_EBIT: 0               NaN\n",
      "1          2.445627\n",
      "2          2.454236\n",
      "3          2.265428\n",
      "4         14.682634\n",
      "            ...    \n",
      "22966     11.380713\n",
      "22967      8.435496\n",
      "22968    -25.810275\n",
      "22969   -768.682712\n",
      "22970   -179.213004\n",
      "Name: EV_EBIT, Length: 22971, dtype: float64\n",
      "Receita_Total_TTM: 0                 NaN\n",
      "1        8.953900e+08\n",
      "2        9.426960e+08\n",
      "3        9.534000e+08\n",
      "4        9.514700e+08\n",
      "             ...     \n",
      "22966    4.015523e+09\n",
      "22967    4.016699e+09\n",
      "22968    4.556360e+09\n",
      "22969    4.686377e+09\n",
      "22970    4.732136e+09\n",
      "Name: Receita_Total_TTM, Length: 22971, dtype: float64\n",
      "EV_Receita: 0             NaN\n",
      "1        0.358897\n",
      "2        0.346138\n",
      "3        0.332519\n",
      "4        1.552597\n",
      "           ...   \n",
      "22966    0.256462\n",
      "22967    0.177551\n",
      "22968    0.201844\n",
      "22969    0.257027\n",
      "22970    0.293239\n",
      "Name: EV_Receita, Length: 22971, dtype: float64\n",
      "Giro_Ativos (preliminar): 0             NaN\n",
      "1        0.529671\n",
      "2        0.538368\n",
      "3        0.530606\n",
      "4        0.486859\n",
      "           ...   \n",
      "22966    0.943680\n",
      "22967    0.908032\n",
      "22968    0.998403\n",
      "22969    1.040874\n",
      "22970    1.056195\n",
      "Name: Giro_Ativos, Length: 22971, dtype: float64\n",
      "Giro_Ativos: 0             NaN\n",
      "1        0.529671\n",
      "2        0.538368\n",
      "3        0.530606\n",
      "4        0.486859\n",
      "           ...   \n",
      "22966    0.943680\n",
      "22967    0.908032\n",
      "22968    0.998403\n",
      "22969    1.040874\n",
      "22970    1.056195\n",
      "Name: Giro_Ativos, Length: 22971, dtype: float64\n",
      "ROIC: 0             NaN\n",
      "1        0.075505\n",
      "2        0.074236\n",
      "3        0.077681\n",
      "4        0.058497\n",
      "           ...   \n",
      "22966    0.024836\n",
      "22967    0.022439\n",
      "22968   -0.009610\n",
      "22969   -0.000410\n",
      "22970   -0.002187\n",
      "Name: ROIC, Length: 22971, dtype: float64\n",
      "ROE: 0             NaN\n",
      "1       -0.002060\n",
      "2       -0.019853\n",
      "3       -0.007754\n",
      "4        0.029008\n",
      "           ...   \n",
      "22966   -0.073722\n",
      "22967   -0.065339\n",
      "22968   -0.133097\n",
      "22969   -0.096704\n",
      "22970   -0.122371\n",
      "Name: ROE, Length: 22971, dtype: float64\n",
      "ROA: 0             NaN\n",
      "1       -0.001008\n",
      "2       -0.009702\n",
      "3       -0.003763\n",
      "4        0.014720\n",
      "           ...   \n",
      "22966   -0.022898\n",
      "22967   -0.020670\n",
      "22968   -0.041922\n",
      "22969   -0.031994\n",
      "22970   -0.042197\n",
      "Name: ROA, Length: 22971, dtype: float64\n",
      "NetDebt_Patrimonio: 0             NaN\n",
      "1        0.388473\n",
      "2        0.381308\n",
      "3        0.363579\n",
      "4        0.144687\n",
      "           ...   \n",
      "22966    0.819527\n",
      "22967    0.777040\n",
      "22968    0.702429\n",
      "22969    0.691597\n",
      "22970    0.512230\n",
      "Name: NetDebt_Patrimonio, Length: 22971, dtype: float64\n",
      "NetDebt_EBIT: 0               NaN\n",
      "1          2.445627\n",
      "2          2.454236\n",
      "3          2.265428\n",
      "4          1.426112\n",
      "            ...    \n",
      "22966     11.969488\n",
      "22967     12.861409\n",
      "22968    -28.336916\n",
      "22969   -657.437779\n",
      "22970   -102.205088\n",
      "Name: NetDebt_EBIT, Length: 22971, dtype: float64\n",
      "Patrimonio_Ativos: 0             NaN\n",
      "1        0.489345\n",
      "2        0.488711\n",
      "3        0.485277\n",
      "4        0.507437\n",
      "           ...   \n",
      "22966    0.310593\n",
      "22967    0.316344\n",
      "22968    0.314977\n",
      "22969    0.330850\n",
      "22970    0.344829\n",
      "Name: Patrimonio_Ativos, Length: 22971, dtype: float64\n",
      "Passivos_Ativos: 0             NaN\n",
      "1        0.510655\n",
      "2        0.511289\n",
      "3        0.514723\n",
      "4        0.492563\n",
      "           ...   \n",
      "22966    0.689407\n",
      "22967    0.683656\n",
      "22968    0.685023\n",
      "22969    0.669150\n",
      "22970    0.655171\n",
      "Name: Passivos_Ativos, Length: 22971, dtype: float64\n",
      "Margem_Liquida_Sector: 0             NaN\n",
      "1       -0.001903\n",
      "2       -0.018022\n",
      "3       -0.007091\n",
      "4        0.030234\n",
      "           ...   \n",
      "22966   -0.024264\n",
      "22967   -0.022763\n",
      "22968   -0.041989\n",
      "22969   -0.030738\n",
      "22970   -0.039952\n",
      "Name: Margem_Liquida_Sector, Length: 22971, dtype: float64\n",
      "Margem_EBIT_Sector: 0             NaN\n",
      "1        0.146751\n",
      "2        0.141037\n",
      "3        0.146780\n",
      "4        0.105744\n",
      "           ...   \n",
      "22966    0.022535\n",
      "22967    0.021048\n",
      "22968   -0.007820\n",
      "22969   -0.000334\n",
      "22970   -0.001636\n",
      "Name: Margem_EBIT_Sector, Length: 22971, dtype: float64\n",
      "Margem_Bruta: 0             NaN\n",
      "1        0.392624\n",
      "2        0.394109\n",
      "3        0.399548\n",
      "4        0.378836\n",
      "           ...   \n",
      "22966    0.665107\n",
      "22967    0.664267\n",
      "22968    0.649304\n",
      "22969    0.647344\n",
      "22970    0.649520\n",
      "Name: Margem_Bruta, Length: 22971, dtype: float64\n",
      "Capital_Giro: 0                 NaN\n",
      "1        5.391500e+06\n",
      "2       -4.483667e+06\n",
      "3       -1.573375e+07\n",
      "4       -5.085000e+05\n",
      "             ...     \n",
      "22966   -4.096425e+07\n",
      "22967    3.383525e+07\n",
      "22968    1.783550e+08\n",
      "22969    1.630470e+08\n",
      "22970    1.217852e+08\n",
      "Name: Capital_Giro, Length: 22971, dtype: float64\n",
      "VPA_calc: 0             NaN\n",
      "1        8.203613\n",
      "2        8.486500\n",
      "3        8.647226\n",
      "4        9.834636\n",
      "           ...   \n",
      "22966    4.799705\n",
      "22967    5.081998\n",
      "22968    3.532369\n",
      "22969    3.660546\n",
      "22970    3.796577\n",
      "Name: VPA_calc, Length: 22971, dtype: float64\n",
      "VPA_calc: 0             NaN\n",
      "1        8.203613\n",
      "2        8.486500\n",
      "3        8.647226\n",
      "4        9.834636\n",
      "           ...   \n",
      "22966    4.799705\n",
      "22967    5.081998\n",
      "22968    3.532369\n",
      "22969    3.660546\n",
      "22970    3.796577\n",
      "Name: VPA_calc, Length: 22971, dtype: float64\n",
      "EBIT_per_share: 0             NaN\n",
      "1        1.303095\n",
      "2        1.318526\n",
      "3        1.387796\n",
      "4        0.997777\n",
      "           ...   \n",
      "22966    0.328626\n",
      "22967    0.307036\n",
      "22968   -0.087562\n",
      "22969   -0.003851\n",
      "22970   -0.019028\n",
      "Name: EBIT_per_share, Length: 22971, dtype: float64\n",
      "Receita_Total_per_share: 0              NaN\n",
      "1         8.879656\n",
      "2         9.348793\n",
      "3         9.454945\n",
      "4         9.435806\n",
      "           ...    \n",
      "22966    14.583053\n",
      "22967    14.587324\n",
      "22968    11.196793\n",
      "22969    11.516296\n",
      "22970    11.628744\n",
      "Name: Receita_Total_per_share, Length: 22971, dtype: float64\n",
      "EBIT_per_share: 0             NaN\n",
      "1        1.303095\n",
      "2        1.318526\n",
      "3        1.387796\n",
      "4        0.997777\n",
      "           ...   \n",
      "22966    0.328626\n",
      "22967    0.307036\n",
      "22968   -0.087562\n",
      "22969   -0.003851\n",
      "22970   -0.019028\n",
      "Name: EBIT_per_share, Length: 22971, dtype: float64\n",
      "Receita_Total_per_share: 0              NaN\n",
      "1         8.879656\n",
      "2         9.348793\n",
      "3         9.454945\n",
      "4         9.435806\n",
      "           ...    \n",
      "22966    14.583053\n",
      "22967    14.587324\n",
      "22968    11.196793\n",
      "22969    11.516296\n",
      "22970    11.628744\n",
      "Name: Receita_Total_per_share, Length: 22971, dtype: float64\n",
      "Receita_Total_per_share: 0              NaN\n",
      "1         8.879656\n",
      "2         9.348793\n",
      "3         9.454945\n",
      "4         9.435806\n",
      "           ...    \n",
      "22966    14.583053\n",
      "22967    14.587324\n",
      "22968    11.196793\n",
      "22969    11.516296\n",
      "22970    11.628744\n",
      "Name: Receita_Total_per_share, Length: 22971, dtype: float64\n",
      "Liquidez_Corrente_Calc: 0             NaN\n",
      "1        1.022203\n",
      "2        0.983231\n",
      "3        0.947507\n",
      "4        0.998564\n",
      "           ...   \n",
      "22966    0.963796\n",
      "22967    1.027920\n",
      "22968    1.152749\n",
      "22969    1.151435\n",
      "22970    1.116478\n",
      "Name: Liquidez_Corrente_Calc, Length: 22971, dtype: float64\n",
      "Payout_TTM: 0       NaN\n",
      "1       NaN\n",
      "2       NaN\n",
      "3       NaN\n",
      "4       NaN\n",
      "         ..\n",
      "22966   NaN\n",
      "22967   NaN\n",
      "22968   NaN\n",
      "22969   NaN\n",
      "22970   NaN\n",
      "Name: Payout_TTM, Length: 22971, dtype: float64\n",
      "Dividend_Yield_TTM: 0       NaN\n",
      "1       NaN\n",
      "2       NaN\n",
      "3       NaN\n",
      "4       NaN\n",
      "         ..\n",
      "22966   NaN\n",
      "22967   NaN\n",
      "22968   NaN\n",
      "22969   NaN\n",
      "22970   NaN\n",
      "Name: Dividend_Yield_TTM, Length: 22971, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_28708\\3510835083.py:844: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: _cagr_windowed(g.set_index(\"FY\")[col])))\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_28708\\3510835083.py:844: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: _cagr_windowed(g.set_index(\"FY\")[col])))\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_28708\\3510835083.py:182: RuntimeWarning: invalid value encountered in scalar power\n",
      "  return (last / first) ** (1.0 / n) - 1.0\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_28708\\3510835083.py:844: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: _cagr_windowed(g.set_index(\"FY\")[col])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base gerada: base_para_simulador_indicadores_refatorado.csv (22971 linhas, 274 colunas)\n",
      "🧾 Relatório de preenchimento: base_para_simulador_indicadores_refatorado_audit.csv\n",
      "✅ Minimal gerada: base_para_simulador_indicadores_refatorado_minimal.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Refatora e amplia o cálculo de indicadores fundamentalistas (TTM e FY),\n",
    "com regras por setor, validação robusta de colunas e funções auxiliares\n",
    "para geração de pontuações (0–100) e redução de colunas para a UI.\n",
    "\n",
    "Assunções fornecidas:\n",
    "- Dívida líquida inclui arrendamentos (IFRS 16).\n",
    "- Payout inclui JCP.\n",
    "- ROE/ROIC com médias de balanço (média dos últimos 4 trimestres).\n",
    "- Preferência por base trimestral (TTM) e FY (ano calendário).\n",
    "- Setores: Basic Materials, Communication Services, Consumer Cyclical,\n",
    "  Consumer Defensive, Energy, Financial Services, Healthcare, Industrials,\n",
    "  Real Estate, Technology, Utilities.\n",
    "\n",
    "Entradas esperadas:\n",
    "- base_unificada_quarterly.csv (múltiplos trimestres por Ticker)\n",
    "- summary_profile.csv (com Setor_Oficial por Ticker)\n",
    "\n",
    "Saídas típicas:\n",
    "- base_para_simulador_indicadores_refatorado.csv (com indicadores)\n",
    "- base_para_simulador_indicadores_refatorado_audit.csv (completude)\n",
    "- (opcionais no main) _minimal.csv e _scores_*.csv\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================\n",
    "# Utilidades\n",
    "# =============================================================\n",
    "\n",
    "def sdiv(num, den):\n",
    "    \"\"\"Divisão segura que retorna NaN quando o denominador é zero/NaN.\"\"\"\n",
    "    try:\n",
    "        num = np.asarray(num, dtype=\"float64\")\n",
    "        den = np.asarray(den, dtype=\"float64\")\n",
    "        out = np.full_like(num, np.nan, dtype=\"float64\")\n",
    "        mask = np.isfinite(num) & np.isfinite(den) & (den != 0)\n",
    "        out[mask] = num[mask] / den[mask]\n",
    "        return out\n",
    "    except Exception:\n",
    "        if den in (0, None) or (isinstance(den, float) and (not math.isfinite(den))):\n",
    "            return np.nan\n",
    "        return num / den\n",
    "\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Retorna o primeiro nome de coluna existente dentre os candidatos.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def classify_financials(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Marca linhas do setor financeiro (case-insensitive).\"\"\"\n",
    "    return series.fillna(\"\").str.contains(\"Financial\", case=False)\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Normalização de colunas (aliases)\n",
    "# =============================================================\n",
    "ALIASES: Dict[str, List[str]] = {\n",
    "    # Saldos (balanço)\n",
    "    \"Ativo_Total_Completo\": [\n",
    "        \"Ativo_Total_Completo\", \"Ativo_Total\", \"Ativo_Total_Calculado\", \"Total_Ativos\"\n",
    "    ],\n",
    "    \"Patrimonio_Liquido_Completo\": [\n",
    "        \"Patrimonio_Liquido_Completo\", \"Patrimonio_Liquido\", \"PL\"\n",
    "    ],\n",
    "    \"Divida_Bruta\": [\n",
    "        \"Divida_Bruta\", \"Divida_Total\", \"Divida_Bruta_IFRS16\"\n",
    "    ],\n",
    "    \"Caixa_e_Investimentos_Completo\": [\n",
    "        \"Caixa_e_Investimentos_Completo\", \"Caixa_e_Equivalentes\", \"Caixa_Total\", \"Caixa\"\n",
    "    ],\n",
    "    \"Ativo_Circulante\": [\n",
    "        \"Ativo_Circulante\"\n",
    "    ],\n",
    "    \"Passivo_Circulante\": [\n",
    "        \"Passivo_Circulante\", \"Passivo_Circulante_Completo\"\n",
    "    ],\n",
    "\n",
    "    # Fluxos (DRE/DFC)\n",
    "    \"Receita_Liquida\": [\n",
    "        \"Receita_Liquida\", \"Receita_Operacional_Liquida\"\n",
    "    ],\n",
    "    \"Receita_Total\": [\n",
    "        \"Receita_Total\", \"Receita_Bruta\", \"Receita\"\n",
    "    ],\n",
    "    \"EBIT\": [\n",
    "        \"EBIT\", \"LAJIR\", \"Lucro_Operacional\"\n",
    "    ],\n",
    "    \"Lucro_Liquido\": [\n",
    "        \"Lucro_Liquido\"\n",
    "    ],\n",
    "    \"Dividendos\": [\n",
    "        \"Dividendos\"\n",
    "    ],\n",
    "    \"Juros_Sobre_Capital_Proprio\": [\n",
    "        \"Juros_Sobre_Capital_Proprio\", \"JCP\"\n",
    "    ],\n",
    "    \"Lucro_Bruto\": [\n",
    "        \"Lucro_Bruto\", \"Lucro_Grosso\", \"Gross_Profit\"\n",
    "    ],\n",
    "\n",
    "    # Mercado\n",
    "    \"Acoes_Emitidas\": [\n",
    "        \"Acoes_Emitidas\", \"Qtde_Acoes\", \"Acoes\"\n",
    "    ],\n",
    "    \"Preco_Atual\": [\n",
    "        \"Preco_Atual\", \"Preco\", \"Close\"\n",
    "    ],\n",
    "    \"Valor_Empresa\": [\n",
    "        \"Valor_Empresa\", \"Enterprise_Value\", \"EV\"\n",
    "    ],\n",
    "\n",
    "    # Perfil\n",
    "    \"Setor_Oficial\": [\n",
    "        \"Setor_Oficial\", \"Setor\", \"Sector\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "FLOW_COLS = [\n",
    "    \"Receita_Liquida\",\n",
    "    \"Receita_Total\",\n",
    "    \"EBIT\",\n",
    "    \"Lucro_Operacional\",\n",
    "    \"Lucro_Liquido\",\n",
    "    \"Dividendos\",\n",
    "    \"Juros_Sobre_Capital_Proprio\",\n",
    "    \"Lucro_Bruto\",\n",
    "]\n",
    "\n",
    "BAL_COLS = [\n",
    "    \"Ativo_Total_Completo\",\n",
    "    \"Patrimonio_Liquido_Completo\",\n",
    "    \"Divida_Bruta\",\n",
    "    \"Caixa_e_Investimentos_Completo\",\n",
    "    \"Ativo_Circulante\",\n",
    "    \"Passivo_Circulante\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cria colunas padronizadas a partir de aliases, quando possível.\"\"\"\n",
    "    for std, cands in ALIASES.items():\n",
    "        if std not in df.columns:\n",
    "            c = pick_col(df, cands)\n",
    "            if c and c != std:\n",
    "                df[std] = df[c]\n",
    "    return df\n",
    "\n",
    "def to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# helpers (coloque junto das outras utilidades, ex.: perto de sdiv/to_num)\n",
    "def _cagr_windowed(series: pd.Series, years: int = 5, min_points: int = 3) -> float:\n",
    "    s = to_num(series).dropna()\n",
    "    if s.size < min_points:\n",
    "        return np.nan\n",
    "    last_year = int(s.index.max())\n",
    "    start_year = last_year - years\n",
    "    s_window = s.loc[s.index >= start_year]\n",
    "    if s_window.empty:\n",
    "        return np.nan\n",
    "    first_year = int(s_window.index.min())\n",
    "    n = last_year - first_year  # número de anos efetivos\n",
    "    if n <= 0:\n",
    "        return np.nan\n",
    "    first = s_window.loc[first_year]\n",
    "    last = s.loc[last_year]\n",
    "    if not np.isfinite(first) or first <= 0 or not np.isfinite(last):\n",
    "        return np.nan\n",
    "    return (last / first) ** (1.0 / n) - 1.0\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Cálculo TTM e FY\n",
    "# =============================================================\n",
    "\n",
    "def compute_ttm_and_fy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"Data_Referencia\"] = pd.to_datetime(df[\"Data_Referencia\"], errors=\"coerce\")\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    # ordena e remove duplicatas por Ticker+Data para evitar que rolling conte linhas duplicadas\n",
    "    df.sort_values([\"Ticker\", \"Data_Referencia\"], inplace=True)\n",
    "    df = df.drop_duplicates(subset=[\"Ticker\", \"Data_Referencia\"], keep=\"last\")\n",
    "\n",
    "    grp = df.groupby(\"Ticker\", group_keys=False)\n",
    "\n",
    "    # garante numérico nas colunas usadas\n",
    "    for col in (FLOW_COLS + BAL_COLS):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # TTM: soma (fluxos) e média (saldos) com índice alinhado via transform\n",
    "    for col in FLOW_COLS:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_TTM\"] = grp[col].transform(lambda s: s.rolling(window=4, min_periods=2).sum())\n",
    "    for col in BAL_COLS:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_AVG4Q\"] = grp[col].transform(lambda s: s.rolling(window=4, min_periods=2).mean())\n",
    "\n",
    "    # Mercado/preço: forward-fill por Ticker com transform('ffill') (evita desalinhamento)\n",
    "    for col in [\"Acoes_Emitidas\", \"Preco_Atual\", \"Valor_Empresa\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = grp[col].transform(\"ffill\")\n",
    "\n",
    "    # FY (ano calendário)\n",
    "    df[\"FY\"] = df[\"Data_Referencia\"].dt.year\n",
    "    # contagem de trimestres por Ticker-Ano (para regra \"estrita\" = exige 4T)\n",
    "    fy_cnt = df.groupby([\"Ticker\", \"FY\"])[\"Data_Referencia\"].transform(\"count\")\n",
    "\n",
    "    # Fluxos: soma do ano; zera/NaN se menos de 4 trimestres (mantido comportamento original)\n",
    "    for col in FLOW_COLS:\n",
    "        if col in df.columns:\n",
    "            s = df.groupby([\"Ticker\", \"FY\"])[col].transform(\"sum\")\n",
    "            s[fy_cnt < 4] = np.nan\n",
    "            df[f\"{col}_FY\"] = s\n",
    "\n",
    "    # Saldos: média do ano; exige pelo menos 2 trimestres (mantido)\n",
    "    for col in BAL_COLS:\n",
    "        if col in df.columns:\n",
    "            s = df.groupby([\"Ticker\", \"FY\"])[col].transform(\"mean\")\n",
    "            s[fy_cnt < 2] = np.nan\n",
    "            df[f\"{col}_FYAVG\"] = s\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Merge de perfil (setor)\n",
    "# =============================================================\n",
    "\n",
    "def merge_sector_profile(df_base: pd.DataFrame, df_profile: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Une Setor_Oficial do profile à base e cria Setor_Oficial_final.\"\"\"\n",
    "    df_profile = df_profile.rename(columns={\"Setor_Oficial\": \"Setor_Oficial_profile\"})\n",
    "    out = df_base.merge(df_profile[[\"Ticker\", \"Setor_Oficial_profile\"]], on=\"Ticker\", how=\"left\")\n",
    "    out[\"Setor_Oficial_final\"] = out.get(\"Setor_Oficial\").fillna(out.get(\"Setor_Oficial_profile\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Auditoria de preenchimento\n",
    "# =============================================================\n",
    "\n",
    "def audit(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Retorna um pequeno relatório de completude dos principais indicadores.\"\"\"\n",
    "    cols = [\n",
    "        \"CAGR5_Lucro\", \"CAGR5_Receita\", \"Capital_Giro\", \"DY_Medio_5anos\", \"Dividend_Yield_TTM\",\n",
    "        \"EBIT_per_share\",\"EBIT\", \"EV_EBIT\", \"EV_Receita\", \"Giro_Ativos\", \"Liquidez_Corrente_Calc\",\n",
    "        \"Lucro_Por_Acao\", \"Margem_Bruta\", \"Margem_EBIT_Sector\", \"Margem_Liquida_Sector\",\n",
    "        \"NetDebt_EBIT\", \"NetDebt_Patrimonio\", \"Net_Debt\", \"P_Ativo\", \"P_CapGiro\", \"P_EBIT\",\n",
    "        \"Preco_Lucro\", \"Preco_PVP\", \"PSR_calc\", \"Passivos_Ativos\", \"Patrimonio_Ativos\", \"Payout_TTM\",\n",
    "        \"ROA\", \"ROE\", \"ROIC\", \"Receita_Total_per_share\"\n",
    "    ]\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Indicador\": present,\n",
    "            \"Nao_Nulos\": [df[c].notna().sum() for c in present],\n",
    "            \"Perc_Preench\": [df[c].notna().mean() * 100 for c in present],\n",
    "        }\n",
    "    ).sort_values(\"Perc_Preench\", ascending=False)\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_columns_for_app(\n",
    "    df: pd.DataFrame,\n",
    "    #indicators: Optional[List[str]] = None,\n",
    "    sector_col: str = \"Setor_Oficial_final\",\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    indicators = [\n",
    "        \"CAGR5_Lucro\", \"CAGR5_Receita\", \"Capital_Giro\", \"DY_Medio_5anos\", \"Dividend_Yield_TTM\",\n",
    "        \"EBIT_per_share\",\"EBIT\", \"EV_EBIT\", \"EV_Receita\", \"Giro_Ativos\", \"Liquidez_Corrente_Calc\",\n",
    "        \"Lucro_Por_Acao\", \"Margem_Bruta\", \"Margem_EBIT_Sector\", \"Margem_Liquida_Sector\",\n",
    "        \"NetDebt_EBIT\", \"NetDebt_Patrimonio\", \"Net_Debt\", \"P_Ativo\", \"P_CapGiro\", \"P_EBIT\",\n",
    "        \"Preco_Lucro\", \"Preco_PVP\", \"PSR_calc\", \"Passivos_Ativos\", \"Patrimonio_Ativos\", \"Payout_TTM\",\n",
    "        \"ROA\", \"ROE\", \"ROIC\", \"Receita_Total_per_share\"\n",
    "    ]\n",
    "    id_cols = [c for c in [\"Ticker\", \"Data_Referencia\", sector_col, \"Preco_Atual\", \"Valor_Empresa\"] if c in df.columns]\n",
    "    keep = id_cols + [c for c in indicators if c in df.columns]\n",
    "    out = df[keep].copy()\n",
    "    num_cols = [c for c in indicators if c in out.columns]\n",
    "    if num_cols:\n",
    "        out[num_cols] = out[num_cols].apply(pd.to_numeric, errors=\"coerce\").round(2)\n",
    "    return out\n",
    "\n",
    "# =============================================================\n",
    "# Audit alias\n",
    "# =============================================================\n",
    "\n",
    "def audit_aliases(\n",
    "    df: pd.DataFrame,\n",
    "    aliases: Dict[str, List[str]] = ALIASES,\n",
    "    tol_abs: float = 1e-6,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retorna (presence_report, choice_report, conflict_report).\n",
    "    - presence_report: linhas por std/alias com contagens de não nulos e % cobertura.\n",
    "    - choice_report: por std, qual alias seria escolhido (1ª prioridade presente) e se há outro melhor.\n",
    "    - conflict_report: por std/par-alias com valores simultaneamente preenchidos e diferentes.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    presence_rows = []\n",
    "    for std, cands in aliases.items():\n",
    "        for rank, col in enumerate(cands, start=1):\n",
    "            present = col in df.columns\n",
    "            nn = int(df[col].notna().sum()) if present else 0\n",
    "            presence_rows.append({\n",
    "                \"std\": std,\n",
    "                \"alias\": col,\n",
    "                \"rank\": rank,\n",
    "                \"present\": present,\n",
    "                \"notnull\": nn,\n",
    "                \"coverage_pct\": (nn / total_rows * 100.0) if total_rows else 0.0,\n",
    "            })\n",
    "    presence_report = pd.DataFrame(presence_rows).sort_values([\"std\",\"rank\"])\n",
    "\n",
    "    choice_rows = []\n",
    "    for std, cands in aliases.items():\n",
    "        # alias escolhido por prioridade\n",
    "        chosen = None\n",
    "        for col in cands:\n",
    "            if col in df.columns and df[col].notna().any():\n",
    "                chosen = col\n",
    "                break\n",
    "        # alias com maior preenchimento (independente de prioridade)\n",
    "        if not presence_report.empty:\n",
    "            pres_std = presence_report[presence_report[\"std\"] == std]\n",
    "            best_row = pres_std.sort_values([\"notnull\",\"present\",\"rank\"], ascending=[False, False, True]).head(1)\n",
    "            best_alias = best_row[\"alias\"].iloc[0] if not best_row.empty else None\n",
    "            best_notnull = int(best_row[\"notnull\"].iloc[0]) if not best_row.empty else 0\n",
    "            best_cov = float(best_row[\"coverage_pct\"].iloc[0]) if not best_row.empty else 0.0\n",
    "        else:\n",
    "            best_alias = None; best_notnull = 0; best_cov = 0.0\n",
    "\n",
    "        chosen_notnull = int(df[chosen].notna().sum()) if (chosen and chosen in df.columns) else 0\n",
    "        chosen_cov = (chosen_notnull / total_rows * 100.0) if total_rows else 0.0\n",
    "\n",
    "        choice_rows.append({\n",
    "            \"std\": std,\n",
    "            \"chosen_alias\": chosen,\n",
    "            \"chosen_notnull\": chosen_notnull,\n",
    "            \"chosen_coverage_pct\": chosen_cov,\n",
    "            \"best_alias_by_data\": best_alias,\n",
    "            \"best_notnull\": best_notnull,\n",
    "            \"best_coverage_pct\": best_cov,\n",
    "            \"alternative_has_more_data\": (best_alias is not None and chosen is not None and best_alias != chosen and best_notnull > chosen_notnull),\n",
    "        })\n",
    "    choice_report = pd.DataFrame(choice_rows).sort_values(\"std\")\n",
    "\n",
    "    # conflitos: quando há 2+ aliases presentes e ambos preenchidos na mesma linha, mas valores diferentes\n",
    "    conflict_rows = []\n",
    "    for std, cands in aliases.items():\n",
    "        present_cands = [c for c in cands if c in df.columns]\n",
    "        if len(present_cands) < 2:\n",
    "            continue\n",
    "        # pares\n",
    "        for i in range(len(present_cands)):\n",
    "            for j in range(i+1, len(present_cands)):\n",
    "                c1, c2 = present_cands[i], present_cands[j]\n",
    "                s1 = pd.to_numeric(df[c1], errors=\"coerce\")\n",
    "                s2 = pd.to_numeric(df[c2], errors=\"coerce\")\n",
    "                both = s1.notna() & s2.notna()\n",
    "                if not both.any():\n",
    "                    continue\n",
    "                diff = (s1[both] - s2[both]).abs()\n",
    "                conflicts = (diff > tol_abs)\n",
    "                num_both = int(both.sum())\n",
    "                num_conf = int(conflicts.sum())\n",
    "                conflict_rows.append({\n",
    "                    \"std\": std,\n",
    "                    \"alias_a\": c1,\n",
    "                    \"alias_b\": c2,\n",
    "                    \"rows_both_filled\": num_both,\n",
    "                    \"rows_conflicting\": num_conf,\n",
    "                    \"conflict_pct_when_both\": (num_conf / num_both * 100.0) if num_both else 0.0,\n",
    "                })\n",
    "    conflict_report = pd.DataFrame(conflict_rows).sort_values([\"std\",\"rows_conflicting\"], ascending=[True, False])\n",
    "\n",
    "    return presence_report, choice_report, conflict_report\n",
    "\n",
    "def audit_aliases_from_csv(\n",
    "    base_csv: str,\n",
    "    out_prefix: str = \"alias_audit\",\n",
    "    aliases: Dict[str, List[str]] = ALIASES,\n",
    "    tol_abs: float = 1e-6,\n",
    ") -> tuple[str, str, str]:\n",
    "    df_base = pd.read_csv(base_csv)\n",
    "    pres, choice, conflict = audit_aliases(df_base, aliases=aliases, tol_abs=tol_abs)\n",
    "\n",
    "    p1 = f\"{out_prefix}_presence.csv\"\n",
    "    p2 = f\"{out_prefix}_choice.csv\"\n",
    "    p3 = f\"{out_prefix}_conflicts.csv\"\n",
    "    pres.to_csv(p1, index=False, encoding=\"utf-8-sig\")\n",
    "    choice.to_csv(p2, index=False, encoding=\"utf-8-sig\")\n",
    "    conflict.to_csv(p3, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# Pipeline\n",
    "# =============================================================\n",
    "\n",
    "def run_pipeline(base_csv: str, profile_csv: str, out_csv: str) -> pd.DataFrame:\n",
    "    df_base = pd.read_csv(base_csv, parse_dates=[\"Data_Referencia\"]) \n",
    "    df_profile = pd.read_csv(profile_csv)\n",
    "    df = merge_sector_profile(df_base, df_profile)\n",
    "    df = compute_ttm_and_fy(df)\n",
    "    df = compute_indicators(df)\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcula indicadores TTM com regras por setor (Financeiro tratado à parte).\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Setor final (base ou profile)\n",
    "    if \"Setor_Oficial_final\" not in df.columns:\n",
    "        df[\"Setor_Oficial_final\"] = df.get(\"Setor_Oficial\").fillna(df.get(\"Setor_Oficial_profile\"))\n",
    "    is_fin = classify_financials(df[\"Setor_Oficial_final\"]) if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    # Dívida Líquida (inclui arrendamentos por assunção)\n",
    "    to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # 1) Dívida: usa Divida_Bruta; se nula/ausente, soma componentes (inclui leasing IFRS16)\n",
    "    debt = to_num(df[\"Divida_Bruta\"]) if \"Divida_Bruta\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    if debt.isna().any():\n",
    "        comp_cols = [c for c in [\n",
    "            \"Emprestimos_Financiamentos\",\"Emprestimos_Financiamentos_Long_Prazo\",\n",
    "            \"Debentures\",\"Debentures_Long_Prazo\",\n",
    "            \"Divida_Curto_Prazo\",\"Divida_Long_Prazo\",\"Divida_Curto_Long_Prazo\"\n",
    "        ] if c in df.columns]\n",
    "        if comp_cols:\n",
    "            debt_alt = df[comp_cols].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1, min_count=1)\n",
    "            debt = debt.fillna(debt_alt)\n",
    "    debt = debt.fillna(0.0)\n",
    "\n",
    "    # 2) Caixa: primeiro disponível\n",
    "    cash_cols = [c for c in [\"Caixa_e_Investimentos_Completo\",\"Caixa_Total\",\"Caixa\"] if c in df.columns]\n",
    "    cash = (pd.concat([to_num(df[c]) for c in cash_cols], axis=1).bfill(axis=1).iloc[:, 0]\n",
    "            if cash_cols else pd.Series(0.0, index=df.index)).fillna(0.0)\n",
    "\n",
    "    # 3) Net Debt\n",
    "    df[\"Net_Debt\"] = debt - cash \n",
    "    print(f\"Net_Debt: {df[\"Net_Debt\"]}\")\n",
    "    \n",
    "    # EBIT base (fallback para bancos se EBIT vier nulo)\n",
    "    # Prioridade: EBIT_TTM -> Lucro_Operacional_TTM -> rolling 4Q de Lucro_Operacional -> rolling 4Q de EBIT\n",
    "    if \"EBIT_TTM\" in df.columns:\n",
    "        ebit_like = pd.to_numeric(df[\"EBIT_TTM\"], errors=\"coerce\")\n",
    "    else:\n",
    "        ebit_like = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    if ebit_like.isna().any() and \"Lucro_Operacional_TTM\" in df.columns:\n",
    "        ebit_like = ebit_like.fillna(pd.to_numeric(df[\"Lucro_Operacional_TTM\"], errors=\"coerce\"))\n",
    "\n",
    "    if ebit_like.isna().any() and \"Lucro_Operacional\" in df.columns:\n",
    "        ebit_alt = (df.groupby(\"Ticker\")[\"Lucro_Operacional\"]\n",
    "                    .transform(lambda s: pd.to_numeric(s, errors=\"coerce\").rolling(4, min_periods=2).sum()))\n",
    "        ebit_like = ebit_like.fillna(ebit_alt)\n",
    "\n",
    "    if ebit_like.isna().any() and \"EBIT\" in df.columns:\n",
    "        ebit_alt2 = (df.groupby(\"Ticker\")[\"EBIT\"]\n",
    "                    .transform(lambda s: pd.to_numeric(s, errors=\"coerce\").rolling(4, min_periods=2).sum()))\n",
    "        ebit_like = ebit_like.fillna(ebit_alt2)\n",
    "\n",
    "    df[\"EBIT\"] = ebit_like\n",
    "\n",
    "    print(f\"EBIT: {df[\"EBIT\"]}\")\n",
    "\n",
    "    # EV: usa Valor_Empresa; se faltar, usa Preço* Ações + Net_Debt\n",
    "    if \"Valor_Empresa\" in df.columns:\n",
    "        EV = pd.to_numeric(df[\"Valor_Empresa\"], errors=\"coerce\")\n",
    "    else:\n",
    "        EV = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    if EV.isna().any():\n",
    "        mcap = (\n",
    "            pd.to_numeric(df[\"Preco_Atual\"], errors=\"coerce\") *\n",
    "            pd.to_numeric(df[\"Acoes_Emitidas\"], errors=\"coerce\")\n",
    "            if {\"Preco_Atual\",\"Acoes_Emitidas\"}.issubset(df.columns) else pd.Series(np.nan, index=df.index)\n",
    "        )\n",
    "        net_debt = pd.to_numeric(df[\"Net_Debt\"], errors=\"coerce\") if \"Net_Debt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "        EV = EV.fillna(mcap.add(net_debt, fill_value=0))\n",
    "    print(f\"EV: {EV}\")\n",
    "    \n",
    "    # EV/EBIT (para todos os setores)\n",
    "    df[\"EV_EBIT\"] = sdiv(EV, ebit_like)\n",
    "    print(f\"EV_EBIT: {df[\"EV_EBIT\"]}\")\n",
    "    print(f\"Receita_Total_TTM: {df[\"Receita_Total_TTM\"]}\")\n",
    "\n",
    "    # EV/Receita: financeiros usam Receita_Total_TTM; demais usam Receita_Liquida_TTM\n",
    "    receita_fin = pd.to_numeric(df[\"Receita_Total_TTM\"], errors=\"coerce\") if \"Receita_Total_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    receita_nfin = pd.to_numeric(df[\"Receita_Liquida_TTM\"], errors=\"coerce\") if \"Receita_Liquida_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    base_receita = pd.Series(np.nan, index=df.index)\n",
    "    if 'is_fin' in locals() or 'is_fin' in globals():\n",
    "        base_receita.loc[is_fin] = receita_fin.loc[is_fin]\n",
    "        base_receita.loc[~is_fin] = receita_nfin.loc[~is_fin]\n",
    "    else:\n",
    "        # fallback: se flag setorial não estiver disponível, prioriza Receita_Liquida_TTM e usa Receita_Total_TTM como reserva\n",
    "        base_receita = receita_nfin.fillna(receita_fin)\n",
    "\n",
    "    df[\"EV_Receita\"] = sdiv(EV, base_receita)\n",
    "    print(f\"EV_Receita: {df[\"EV_Receita\"]}\")\n",
    "    \n",
    "    # Giro de Ativos (não comparável em bancos; para bancos usar Receita_Total)\n",
    "    if {\"Ativo_Total_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        if \"Receita_Liquida_TTM\" in df.columns:\n",
    "            df[\"Giro_Ativos\"] = np.where(\n",
    "                is_fin,\n",
    "                sdiv(df[\"Receita_Liquida_TTM\"], df[\"Ativo_Total_Completo_AVG4Q\"]),\n",
    "                sdiv(df.get(\"Receita_Total_TTM\", np.nan), df[\"Ativo_Total_Completo_AVG4Q\"]),\n",
    "            )\n",
    "    print(f\"Giro_Ativos (preliminar): {df.get(\"Giro_Ativos\")}\")\n",
    "    \n",
    "    # Giro de Ativos\n",
    "    is_fin = df[\"Setor_Oficial_final\"].eq(\"Financial Services\") if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    # ativo médio (com fallback)\n",
    "    asset_cols = [\n",
    "        \"Ativo_Total_Completo_AVG4Q\", \"Ativo_Total_AVG4Q\",\n",
    "        \"Total_Ativos_AVG4Q\", \"Ativo_Total_Calculado_AVG4Q\"\n",
    "    ]\n",
    "    ativo_avg = None\n",
    "    for c in asset_cols:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            ativo_avg = s if ativo_avg is None else ativo_avg.combine_first(s)\n",
    "\n",
    "    receita_fin = pd.to_numeric(df[\"Receita_Total_TTM\"], errors=\"coerce\") if \"Receita_Total_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    receita_nfin = pd.to_numeric(df[\"Receita_Liquida_TTM\"], errors=\"coerce\") if \"Receita_Liquida_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    base_receita = pd.Series(np.nan, index=df.index)\n",
    "    base_receita.loc[is_fin] = receita_fin.loc[is_fin]\n",
    "    base_receita.loc[~is_fin] = receita_nfin.loc[~is_fin]\n",
    "\n",
    "    df[\"Giro_Ativos\"] = sdiv(base_receita, ativo_avg)\n",
    "    print(f\"Giro_Ativos: {df[\"Giro_Ativos\"]}\")\n",
    "    \n",
    "    # ROIC\n",
    "    # Capital investido: PL médio 4T + Dívida Líquida\n",
    "    pl_cols = [\n",
    "        \"Patrimonio_Liquido_Completo_AVG4Q\", \"Patrimonio_Liquido_AVG4Q\",\n",
    "        \"PL_AVG4Q\", \"Patrimonio_Liquido_FYAVG\"  # último como reserva\n",
    "    ]\n",
    "    pl_avg = None\n",
    "    for c in pl_cols:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            pl_avg = s if pl_avg is None else pl_avg.combine_first(s)\n",
    "\n",
    "    net_debt = pd.to_numeric(df[\"Net_Debt\"], errors=\"coerce\") if \"Net_Debt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    capital = (pl_avg if pl_avg is not None else pd.Series(np.nan, index=df.index)).add(net_debt, fill_value=0)\n",
    "\n",
    "    # EBIT-like (já calculado antes e salvo em df[\"EBIT\"]; se faltar, tenta TTM)\n",
    "    if \"EBIT\" in df.columns:\n",
    "        ebit_like = pd.to_numeric(df[\"EBIT\"], errors=\"coerce\")\n",
    "    else:\n",
    "        ebit_like = pd.to_numeric(df[\"EBIT_TTM\"], errors=\"coerce\") if \"EBIT_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    # Taxa efetiva (apenas p/ não-fin)\n",
    "    eff_tax = df.get(\"Taxa_Efetiva\")\n",
    "    if isinstance(eff_tax, pd.Series):\n",
    "        eff_tax = pd.to_numeric(eff_tax, errors=\"coerce\").clip(0, 1)\n",
    "    else:\n",
    "        eff_tax = 0.34\n",
    "\n",
    "    nopat = ebit_like * (1 - eff_tax)\n",
    "\n",
    "    lucro_ttm = pd.to_numeric(df[\"Lucro_Liquido_TTM\"], errors=\"coerce\") if \"Lucro_Liquido_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    roic_nonfin = sdiv(nopat, capital)\n",
    "    roic_fin = sdiv(lucro_ttm, capital)\n",
    "\n",
    "    df[\"ROIC\"] = np.where(is_fin, roic_fin, roic_nonfin)\n",
    "    print(f\"ROIC: {df[\"ROIC\"]}\")\n",
    "\n",
    "    # ROE e ROA (TTM)\n",
    "    if {\"Lucro_Liquido_TTM\", \"Patrimonio_Liquido_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        df[\"ROE\"] = sdiv(df[\"Lucro_Liquido_TTM\"], df[\"Patrimonio_Liquido_Completo_AVG4Q\"])\n",
    "    print(f\"ROE: {df[\"ROE\"]}\")\n",
    "    if {\"Lucro_Liquido_TTM\", \"Ativo_Total_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        df[\"ROA\"] = sdiv(df[\"Lucro_Liquido_TTM\"], df[\"Ativo_Total_Completo_AVG4Q\"])\n",
    "    print(f\"ROA: {df[\"ROA\"]}\")\n",
    "\n",
    "    # Alavancagem\n",
    "    if {\"Net_Debt\", \"Patrimonio_Liquido_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        df[\"NetDebt_Patrimonio\"] = sdiv(df[\"Net_Debt\"], df[\"Patrimonio_Liquido_Completo_AVG4Q\"])\n",
    "    print(f\"NetDebt_Patrimonio: {df.get(\"NetDebt_Patrimonio\")}\")\n",
    "    \n",
    "    df[\"NetDebt_EBIT\"] = sdiv(df.get(\"Net_Debt\", np.nan), ebit_like)\n",
    "    print(f\"NetDebt_EBIT: {df.get(\"NetDebt_EBIT\")}\")\n",
    "    \n",
    "    # Estrutura de capital\n",
    "    if {\"Patrimonio_Liquido_Completo_AVG4Q\", \"Ativo_Total_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        df[\"Patrimonio_Ativos\"] = sdiv(df[\"Patrimonio_Liquido_Completo_AVG4Q\"], df[\"Ativo_Total_Completo_AVG4Q\"])\n",
    "        df[\"Passivos_Ativos\"] = 1 - df[\"Patrimonio_Ativos\"]\n",
    "    print(f\"Patrimonio_Ativos: {df.get(\"Patrimonio_Ativos\")}\")\n",
    "    print(f\"Passivos_Ativos: {df.get(\"Passivos_Ativos\")}\")\n",
    "\n",
    "    # Margens (setor-aware)\n",
    "    to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_fin = df[\"Setor_Oficial_final\"].eq(\"Financial Services\") if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    # Bases de receita por setor\n",
    "    rec_fin  = to_num(df[\"Receita_Total_TTM\"])   if \"Receita_Total_TTM\"   in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    rec_nfin = to_num(df[\"Receita_Liquida_TTM\"]) if \"Receita_Liquida_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    base_receita = pd.Series(np.nan, index=df.index)\n",
    "    base_receita.loc[is_fin]  = rec_fin.loc[is_fin]\n",
    "    base_receita.loc[~is_fin] = rec_nfin.loc[~is_fin]\n",
    "\n",
    "    # Margem Líquida (fin: Receita_Total; não-fin: Receita_Liquida)\n",
    "    if \"Lucro_Liquido_TTM\" in df.columns:\n",
    "        df[\"Margem_Liquida_Sector\"] = sdiv(to_num(df[\"Lucro_Liquido_TTM\"]), base_receita)\n",
    "    print(f\"Margem_Liquida_Sector: {df.get(\"Margem_Liquida_Sector\")}\")\n",
    "    # Margem EBIT (não-fin apenas; para fin = NaN)\n",
    "    ebit_like = to_num(df[\"EBIT\"]) if \"EBIT\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    base_mebit = pd.Series(np.nan, index=df.index)\n",
    "    base_mebit.loc[~is_fin] = rec_nfin.loc[~is_fin]\n",
    "\n",
    "    df[\"Margem_EBIT_Sector\"] = sdiv(ebit_like, base_mebit)\n",
    "    print(f\"Margem_EBIT_Sector: {df.get(\"Margem_EBIT_Sector\")}\")\n",
    "\n",
    "    # Margem Bruta (só para não-fin, quando disponível)\n",
    "    if {\"Lucro_Bruto_TTM\", \"Receita_Liquida_TTM\"}.issubset(df.columns):\n",
    "        margem_bruta = sdiv(to_num(df[\"Lucro_Bruto_TTM\"]), rec_nfin)\n",
    "        df[\"Margem_Bruta\"] = np.where(is_fin, np.nan, margem_bruta)\n",
    "    print(f\"Margem_Bruta: {df.get(\"Margem_Bruta\")}\")\n",
    "    \n",
    "    #Divida Líquida/Ativos\n",
    "    if {\"Net_Debt\", \"Ativo_Total_Completo_AVG4Q\"}.issubset(df.columns):\n",
    "        df[\"P_Ativo\"] = sdiv(df[\"Net_Debt\"], df[\"Ativo_Total_Completo_AVG4Q\"])\n",
    "    print(f\"P_Ativo: {df.get('P_Ativo')}\")\n",
    "\n",
    "    # Capital de Giro (médias 4Q)\n",
    "    to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # Fallbacks para AVG4Q (se não existir, tenta derivar do trimestral com rolling 4)\n",
    "    a_circ_avg = None\n",
    "    if \"Ativo_Circulante_AVG4Q\" in df.columns:\n",
    "        a_circ_avg = to_num(df[\"Ativo_Circulante_AVG4Q\"])\n",
    "    elif \"Ativo_Circulante\" in df.columns:\n",
    "        a_circ_avg = (df.groupby(\"Ticker\")[\"Ativo_Circulante\"]\n",
    "                        .transform(lambda s: to_num(s).rolling(4, min_periods=2).mean()))\n",
    "\n",
    "    p_circ_avg = None\n",
    "    if \"Passivo_Circulante_AVG4Q\" in df.columns:\n",
    "        p_circ_avg = to_num(df[\"Passivo_Circulante_AVG4Q\"])\n",
    "    elif \"Passivo_Circulante\" in df.columns:\n",
    "        p_circ_avg = (df.groupby(\"Ticker\")[\"Passivo_Circulante\"]\n",
    "                        .transform(lambda s: to_num(s).rolling(4, min_periods=2).mean()))\n",
    "\n",
    "    if a_circ_avg is not None and p_circ_avg is not None:\n",
    "        df[\"Capital_Giro\"] = a_circ_avg - p_circ_avg\n",
    "    print(f\"Capital_Giro: {df.get(\"Capital_Giro\")}\")\n",
    "\n",
    "    # Indicadores por ação\n",
    "    to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_fin = df[\"Setor_Oficial_final\"].eq(\"Financial Services\") if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    # Ações (usa último disponível por ticker, já ffill na etapa de TTM/FY)\n",
    "    shares = to_num(df[\"Acoes_Emitidas\"]) if \"Acoes_Emitidas\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    shares = shares.replace(0, np.nan)\n",
    "\n",
    "    # VPA e LPA (médias/TTM coerentes)\n",
    "    if \"Patrimonio_Liquido_Completo_AVG4Q\" in df.columns:\n",
    "        df[\"VPA_calc\"] = sdiv(to_num(df[\"Patrimonio_Liquido_Completo_AVG4Q\"]), shares)\n",
    "    print(f\"VPA_calc: {df.get('VPA_calc')}\")\n",
    "    if \"Lucro_Liquido_TTM\" in df.columns:\n",
    "        df[\"LPA_calc\"] = sdiv(to_num(df[\"Lucro_Liquido_TTM\"]), shares)\n",
    "    print(f\"VPA_calc: {df.get(\"VPA_calc\")}\")\n",
    "\n",
    "    # EBIT por ação (usa EBIT “like” já consolidado em df[\"EBIT\"]; fallback mínimo)\n",
    "    if \"EBIT\" in df.columns:\n",
    "        ebit_like = to_num(df[\"EBIT\"])\n",
    "    else:\n",
    "        ebit_like = pd.Series(np.nan, index=df.index)\n",
    "        if \"EBIT_TTM\" in df.columns:\n",
    "            ebit_like = ebit_like.fillna(to_num(df[\"EBIT_TTM\"]))\n",
    "\n",
    "    df[\"EBIT_per_share\"] = sdiv(ebit_like, shares)\n",
    "    print(f\"EBIT_per_share: {df.get(\"EBIT_per_share\")}\")\n",
    "\n",
    "    # Receita por ação (coluna principal setorial-aware)\n",
    "    rec_fin  = to_num(df[\"Receita_Total_TTM\"])   if \"Receita_Total_TTM\"   in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    rec_nfin = to_num(df[\"Receita_Liquida_TTM\"]) if \"Receita_Liquida_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    base_receita = pd.Series(np.nan, index=df.index)\n",
    "    base_receita.loc[is_fin]  = rec_fin.loc[is_fin]\n",
    "    base_receita.loc[~is_fin] = rec_nfin.loc[~is_fin]\n",
    "\n",
    "    df[\"Receita_Total_per_share\"] = sdiv(base_receita, shares)\n",
    "\n",
    "    # (opcional) também expor receita líquida por ação quando existir\n",
    "    if \"Receita_Liquida_TTM\" in df.columns:\n",
    "        df[\"Receita_Liquida_per_share\"] = sdiv(rec_nfin, shares)\n",
    "    print(f\"Receita_Total_per_share: {df.get(\"Receita_Total_per_share\")}\")\n",
    "    \n",
    "    # Fallbacks para AVG4Q (se não existir, tenta derivar do trimestral com rolling 4)\n",
    "    a_circ_avg = None\n",
    "    if \"Ativo_Circulante_AVG4Q\" in df.columns:\n",
    "        a_circ_avg = to_num(df[\"Ativo_Circulante_AVG4Q\"])\n",
    "    elif \"Ativo_Circulante\" in df.columns:\n",
    "        a_circ_avg = (df.groupby(\"Ticker\")[\"Ativo_Circulante\"]\n",
    "                        .transform(lambda s: to_num(s).rolling(4, min_periods=2).mean()))\n",
    "\n",
    "    p_circ_avg = None\n",
    "    if \"Passivo_Circulante_AVG4Q\" in df.columns:\n",
    "        p_circ_avg = to_num(df[\"Passivo_Circulante_AVG4Q\"])\n",
    "    elif \"Passivo_Circulante\" in df.columns:\n",
    "        p_circ_avg = (df.groupby(\"Ticker\")[\"Passivo_Circulante\"]\n",
    "                        .transform(lambda s: to_num(s).rolling(4, min_periods=2).mean()))\n",
    "    \n",
    "    # Indicadores por ação\n",
    "    to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "    is_fin = df[\"Setor_Oficial_final\"].eq(\"Financial Services\") if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    # Ações (usa último disponível por ticker, já ffill na etapa de TTM/FY)\n",
    "    shares = to_num(df[\"Acoes_Emitidas\"]) if \"Acoes_Emitidas\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    shares = shares.replace(0, np.nan)\n",
    "\n",
    "    # VPA e LPA (médias/TTM coerentes)\n",
    "    if \"Patrimonio_Liquido_Completo_AVG4Q\" in df.columns:\n",
    "        df[\"VPA_calc\"] = sdiv(to_num(df[\"Patrimonio_Liquido_Completo_AVG4Q\"]), shares)\n",
    "    if \"Lucro_Liquido_TTM\" in df.columns:\n",
    "        df[\"LPA_calc\"] = sdiv(to_num(df[\"Lucro_Liquido_TTM\"]), shares)\n",
    "\n",
    "    # EBIT por ação (usa EBIT “like” já consolidado em df[\"EBIT\"]; fallback mínimo)\n",
    "    if \"EBIT\" in df.columns:\n",
    "        ebit_like = to_num(df[\"EBIT\"])\n",
    "    else:\n",
    "        ebit_like = pd.Series(np.nan, index=df.index)\n",
    "        if \"EBIT_TTM\" in df.columns:\n",
    "            ebit_like = ebit_like.fillna(to_num(df[\"EBIT_TTM\"]))\n",
    "\n",
    "    df[\"EBIT_per_share\"] = sdiv(ebit_like, shares)\n",
    "    print(f\"EBIT_per_share: {df.get(\"EBIT_per_share\")}\")\n",
    "    # Receita por ação (coluna principal setorial-aware)\n",
    "    rec_fin  = to_num(df[\"Receita_Total_TTM\"])   if \"Receita_Total_TTM\"   in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    rec_nfin = to_num(df[\"Receita_Liquida_TTM\"]) if \"Receita_Liquida_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    base_receita = pd.Series(np.nan, index=df.index)\n",
    "    base_receita.loc[is_fin]  = rec_fin.loc[is_fin]\n",
    "    base_receita.loc[~is_fin] = rec_nfin.loc[~is_fin]\n",
    "\n",
    "    df[\"Receita_Total_per_share\"] = sdiv(base_receita, shares)\n",
    "    print(f\"Receita_Total_per_share: {df.get(\"Receita_Total_per_share\")}\")\n",
    "    # (opcional) também expor receita líquida por ação quando existir\n",
    "    if \"Receita_Liquida_TTM\" in df.columns:\n",
    "        df[\"Receita_Liquida_per_share\"] = sdiv(rec_nfin, shares)\n",
    "    print(f\"Receita_Total_per_share: {df.get(\"Receita_Total_per_share\")}\")\n",
    "    \n",
    "    # Liquidez Corrente (média 4T, com fallback para rolling de trimestral)\n",
    "    pl_to_num = to_num  # reutiliza seu helper\n",
    "\n",
    "    a_circ_avg = None\n",
    "    if \"Ativo_Circulante_AVG4Q\" in df.columns:\n",
    "        a_circ_avg = pl_to_num(df[\"Ativo_Circulante_AVG4Q\"])\n",
    "    elif \"Ativo_Circulante\" in df.columns:\n",
    "        a_circ_avg = (df.groupby(\"Ticker\")[\"Ativo_Circulante\"]\n",
    "                        .transform(lambda s: pl_to_num(s).rolling(4, min_periods=2).mean()))\n",
    "\n",
    "    p_circ_avg = None\n",
    "    if \"Passivo_Circulante_AVG4Q\" in df.columns:\n",
    "        p_circ_avg = pl_to_num(df[\"Passivo_Circulante_AVG4Q\"])\n",
    "    elif \"Passivo_Circulante\" in df.columns:\n",
    "        p_circ_avg = (df.groupby(\"Ticker\")[\"Passivo_Circulante\"]\n",
    "                        .transform(lambda s: pl_to_num(s).rolling(4, min_periods=2).mean()))\n",
    "\n",
    "    if a_circ_avg is not None and p_circ_avg is not None:\n",
    "        df[\"Liquidez_Corrente_Calc\"] = sdiv(a_circ_avg, p_circ_avg)\n",
    "    print(f\"Liquidez_Corrente_Calc: {df.get('Liquidez_Corrente_Calc')}\")\n",
    "    \n",
    "    # Payout (TTM), incluindo JCP\n",
    "    div_ttm = pl_to_num(df[\"Dividendos_TTM\"]) if \"Dividendos_TTM\" in df.columns else pd.Series(0.0, index=df.index)\n",
    "    jcp_ttm = pl_to_num(df[\"Juros_Sobre_Capital_Proprio_TTM\"]) if \"Juros_Sobre_Capital_Proprio_TTM\" in df.columns else pd.Series(0.0, index=df.index)\n",
    "    lucro_ttm = pl_to_num(df[\"Lucro_Liquido_TTM\"]) if \"Lucro_Liquido_TTM\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    df[\"Payout_TTM\"] = sdiv(div_ttm.add(jcp_ttm, fill_value=0), lucro_ttm)\n",
    "    print(f\"Payout_TTM: {df.get('Payout_TTM')}\")\n",
    "    \n",
    "    # Dividend Yield (TTM)\n",
    "    price  = pl_to_num(df[\"Preco_Atual\"]) if \"Preco_Atual\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    shares = pl_to_num(df[\"Acoes_Emitidas\"]) if \"Acoes_Emitidas\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    shares = shares.replace(0, np.nan)\n",
    "\n",
    "    dps_ttm = sdiv(div_ttm.add(jcp_ttm, fill_value=0), shares)\n",
    "    df[\"Dividend_Yield_TTM\"] = sdiv(dps_ttm, price)\n",
    "    print(f\"Dividend_Yield_TTM: {df.get('Dividend_Yield_TTM')}\")\n",
    "    \n",
    "    # DY médio 5 anos (por FY) — calcula anual primeiro e só então faz a média móvel\n",
    "    if {\"FY\", \"Dividendos_FY\", \"Juros_Sobre_Capital_Proprio_FY\"}.issubset(df.columns) and \"Preco_Atual\" in df.columns:\n",
    "        annual = (df.groupby([\"Ticker\", \"FY\"], as_index=False)\n",
    "                    .agg({\n",
    "                        \"Dividendos_FY\": \"last\",\n",
    "                        \"Juros_Sobre_Capital_Proprio_FY\": \"last\",\n",
    "                        \"Acoes_Emitidas\": \"last\",\n",
    "                        \"Preco_Atual\": \"last\",\n",
    "                    }))\n",
    "\n",
    "        annual[\"Acoes_Emitidas\"] = pl_to_num(annual[\"Acoes_Emitidas\"]).replace(0, np.nan)\n",
    "        annual[\"Dividendos_FY\"] = pl_to_num(annual[\"Dividendos_FY\"])\n",
    "        annual[\"Juros_Sobre_Capital_Proprio_FY\"] = pl_to_num(annual[\"Juros_Sobre_Capital_Proprio_FY\"])\n",
    "        annual[\"Preco_Atual\"] = pl_to_num(annual[\"Preco_Atual\"])\n",
    "\n",
    "        annual[\"DPS_FY\"] = sdiv(annual[\"Dividendos_FY\"].add(annual[\"Juros_Sobre_Capital_Proprio_FY\"], fill_value=0), annual[\"Acoes_Emitidas\"])\n",
    "        annual[\"DY_FY\"]  = sdiv(annual[\"DPS_FY\"], annual[\"Preco_Atual\"])\n",
    "\n",
    "        annual = annual.sort_values([\"Ticker\", \"FY\"])\n",
    "        annual[\"DY_Medio_5anos\"] = (annual.groupby(\"Ticker\")[\"DY_FY\"]\n",
    "                                        .transform(lambda s: s.rolling(5, min_periods=3).mean()))\n",
    "\n",
    "        df = df.merge(annual[[\"Ticker\", \"FY\", \"DY_Medio_5anos\"]], on=[\"Ticker\", \"FY\"], how=\"left\")\n",
    "\n",
    "    # CAGR 5 anos de Receita (setor-aware) e de Lucro, via FY\n",
    "    if \"FY\" in df.columns:\n",
    "        cols_needed = [\"Ticker\", \"FY\", \"Setor_Oficial_final\",\n",
    "                    \"Receita_Total_FY\", \"Receita_Liquida_FY\", \"Lucro_Liquido_FY\"]\n",
    "        annual = (df[[c for c in cols_needed if c in df.columns]]\n",
    "                    .drop_duplicates([\"Ticker\", \"FY\"])\n",
    "                    .sort_values([\"Ticker\", \"FY\"]))\n",
    "\n",
    "        def _build_cagr_map(annual_df: pd.DataFrame, col: str) -> pd.Series:\n",
    "            if col not in annual_df.columns:\n",
    "                return pd.Series(dtype=float)\n",
    "            tmp = annual_df[[\"Ticker\", \"FY\", col]].dropna()\n",
    "            if tmp.empty:\n",
    "                return pd.Series(dtype=float)\n",
    "            return (tmp.groupby(\"Ticker\", sort=False)\n",
    "                        .apply(lambda g: _cagr_windowed(g.set_index(\"FY\")[col])))\n",
    "\n",
    "        cagr_rev_total = _build_cagr_map(annual, \"Receita_Total_FY\")\n",
    "        cagr_rev_liq   = _build_cagr_map(annual, \"Receita_Liquida_FY\")\n",
    "        cagr_lucro     = _build_cagr_map(annual, \"Lucro_Liquido_FY\")\n",
    "\n",
    "        is_fin = df[\"Setor_Oficial_final\"].eq(\"Financial Services\") \\\n",
    "                if \"Setor_Oficial_final\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "        df[\"CAGR5_Receita\"] = np.nan\n",
    "        if not cagr_rev_total.empty or not cagr_rev_liq.empty:\n",
    "            df.loc[is_fin,  \"CAGR5_Receita\"] = df.loc[is_fin,  \"Ticker\"].map(cagr_rev_total)\n",
    "            df.loc[~is_fin, \"CAGR5_Receita\"] = df.loc[~is_fin, \"Ticker\"].map(cagr_rev_liq)\n",
    "\n",
    "        df[\"CAGR5_Lucro\"] = df[\"Ticker\"].map(cagr_lucro) if not cagr_lucro.empty else np.nan\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "# =============================================================\n",
    "# Main solicitado\n",
    "# =============================================================\n",
    "\n",
    "def main():\n",
    "    # Execução simples, conforme solicitado\n",
    "    df = run_pipeline(\n",
    "        \"base_unificada_quarterly.csv\",\n",
    "        \"summary_profile.csv\",\n",
    "        \"base_para_simulador_indicadores_refatorado.csv\",\n",
    "    )\n",
    "    print(\n",
    "        f\"Base gerada: base_para_simulador_indicadores_refatorado.csv ({df.shape[0]} linhas, {df.shape[1]} colunas)\"\n",
    "    )\n",
    "\n",
    "    rep = audit(df)\n",
    "    rep.to_csv(\n",
    "        \"base_para_simulador_indicadores_refatorado.csv\".replace(\".csv\", \"_audit.csv\"),\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",\n",
    "    )\n",
    "    print(\n",
    "        f\"🧾 Relatório de preenchimento: {'base_para_simulador_indicadores_refatorado.csv'.replace('.csv','_audit.csv')}\"\n",
    "    )\n",
    "\n",
    "    # (Opcional) já salvar versões minimal e scores global/por setor\n",
    "    try:\n",
    "        df_min = filter_columns_for_app(df)\n",
    "        df_min.to_csv(\n",
    "            \"base_para_simulador_indicadores_refatorado_minimal.csv\",\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "        )\n",
    "        print(\"Minimal gerada: base_para_simulador_indicadores_refatorado_minimal.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"(Aviso) Não foi possível gerar extras minimal/scores: {e}\")\n",
    "        \n",
    "    # Saídas:\n",
    "    # - alias_audit_presence.csv   -> std/alias, presença, não nulos, % cobertura, prioridade\n",
    "    # - alias_audit_choice.csv     -> alias escolhido pela prioridade e o “melhor por dados”\n",
    "    # - alias_audit_conflicts.csv  -> onde dois aliases coexistem e divergem (pistas de inconsistência)\n",
    "    audit_aliases_from_csv(\"base_unificada_quarterly.csv\", out_prefix=\"alias_audit\")\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_BASE = \"https://brapi.dev/api/quote\"\n",
    "OUTDIR = \"benchmark_indices\"  # pasta de saída\n",
    "\n",
    "# ---------- utilitários ----------\n",
    "def _to_datetime_utc(series):\n",
    "    \"\"\"Converte epoch (s ou ms) -> datetime UTC.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    unit = \"ms\" if s.dropna().gt(10**12).any() else \"s\"\n",
    "    return pd.to_datetime(s, unit=unit, utc=True, errors=\"coerce\")\n",
    "\n",
    "def _normalize_hist_df(hist):\n",
    "    \"\"\"Normaliza o historicalDataPrice da brapi para um DataFrame 'padrão'.\"\"\"\n",
    "    df = pd.DataFrame(hist)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    ren = {\n",
    "        \"date\": \"Data\",\n",
    "        \"open\": \"Abertura\",\n",
    "        \"high\": \"Maxima\",\n",
    "        \"low\": \"Minima\",\n",
    "        \"close\": \"Fechamento\",\n",
    "        \"adjustedClose\": \"Fechamento_Ajustado\",\n",
    "        \"adjClose\": \"Fechamento_Ajustado\",\n",
    "        \"volume\": \"Volume\",\n",
    "    }\n",
    "    df = df.rename(columns=ren)\n",
    "    if \"Data\" in df.columns:\n",
    "        df[\"Data\"] = _to_datetime_utc(df[\"Data\"])\n",
    "    # Garante coluna de ajustado\n",
    "    if \"Fechamento_Ajustado\" not in df.columns and \"Fechamento\" in df.columns:\n",
    "        df[\"Fechamento_Ajustado\"] = df[\"Fechamento\"]\n",
    "    return df\n",
    "\n",
    "def fetch_index_with_fallback(symbol: str, token: str = TOKEN, range_param=\"max\",\n",
    "                              intervals=(\"3mo\", \"1mo\", \"1d\"), min_rows=10,\n",
    "                              max_retries=1, sleep_sec=0.8):\n",
    "    \"\"\"\n",
    "    Busca histórico testando múltiplos 'interval'. Retorna (df_hist, raw_json, intervalo_usado).\n",
    "    - intervals: ordem de tentativa.\n",
    "    - min_rows: se linhas < min_rows, tenta próximo intervalo mais granular.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/{symbol}\"\n",
    "    last_payload = {}\n",
    "    for interval in intervals:\n",
    "        params = {\"range\": range_param, \"interval\": interval, \"token\": token}\n",
    "        for _ in range(max_retries + 1):\n",
    "            r = requests.get(url, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            payload = r.json()\n",
    "            last_payload = payload\n",
    "            results = payload.get(\"results\") or []\n",
    "            hist = results[0].get(\"historicalDataPrice\", []) if results else []\n",
    "            df = _normalize_hist_df(hist)\n",
    "            # Se tem dados e quantidade aceitável, usa esse intervalo\n",
    "            if not df.empty and len(df) >= min_rows:\n",
    "                return df, payload, interval\n",
    "            time.sleep(sleep_sec)\n",
    "        # se terminou as tentativas deste interval sem atingir min_rows, cai para o próximo\n",
    "    # último recurso: retorna o que tiver do último intervalo\n",
    "    df = _normalize_hist_df(\n",
    "        (last_payload.get(\"results\") or [{}])[0].get(\"historicalDataPrice\", [])\n",
    "    )\n",
    "    return df, last_payload, intervals[-1]\n",
    "\n",
    "def to_quarterly(df_hist: pd.DataFrame, source_interval: str) -> pd.DataFrame:\n",
    "    \"\"\"Garante granularidade trimestral (se vier 1mo/1d, reamostra para Q no fim do trimestre).\"\"\"\n",
    "    if df_hist.empty:\n",
    "        return df_hist\n",
    "    df = df_hist.sort_values(\"Data\").copy()\n",
    "    if source_interval == \"3mo\":\n",
    "        df_q = df.copy()\n",
    "    else:\n",
    "        # Reamostra para o último valor de cada trimestre (fim do trimestre calendário)\n",
    "        df_q = (\n",
    "            df.set_index(\"Data\")\n",
    "              .resample(\"Q\")\n",
    "              .last()\n",
    "              .dropna(subset=[\"Fechamento_Ajustado\"])\n",
    "              .reset_index()\n",
    "        )\n",
    "    # Data_Referencia como data local no fim do trimestre\n",
    "    try:\n",
    "        df_q[\"Data_Referencia\"] = df_q[\"Data\"].dt.tz_convert(\"America/Sao_Paulo\").dt.date\n",
    "    except Exception:\n",
    "        df_q[\"Data_Referencia\"] = df_q[\"Data\"].dt.tz_localize(\"UTC\").dt.tz_convert(\"America/Sao_Paulo\").dt.date\n",
    "    # Retorno trimestral\n",
    "    df_q[\"Retorno_Trimestral\"] = df_q[\"Fechamento_Ajustado\"].pct_change()\n",
    "    return df_q\n",
    "\n",
    "def save_raw_json(symbol: str, interval_used: str, payload: dict):\n",
    "    os.makedirs(OUTDIR, exist_ok=True)\n",
    "    path = os.path.join(OUTDIR, f\"raw_{symbol.replace('^','').replace('.','_')}_{interval_used}.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"JSON bruto salvo: {path}\")\n",
    "\n",
    "def save_quarterly_csv(symbol: str, df_q: pd.DataFrame):\n",
    "    if df_q.empty:\n",
    "        print(f\" Sem dados suficientes para {symbol}. CSV não gerado.\")\n",
    "        return\n",
    "    df_q = df_q[df_q[\"Data_Referencia\"] >= pd.to_datetime(\"2010-01-01\").date()].copy()\n",
    "    df_q.insert(0, \"Ticker\", symbol)\n",
    "    cols = [\"Ticker\", \"Data_Referencia\", \"Fechamento_Ajustado\", \"Retorno_Trimestral\",\n",
    "            \"Abertura\", \"Maxima\", \"Minima\", \"Volume\"]\n",
    "    cols = [c for c in cols if c in df_q.columns]\n",
    "    csv_path = os.path.join(OUTDIR, f\"{symbol.replace('^','').replace('.','_')}_quarterly_desde2010.csv\")\n",
    "    df_q[cols].sort_values(\"Data_Referencia\").to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"CSV salvo: {csv_path} | linhas: {len(df_q)}\")\n",
    "\n",
    "# ---------- execução ----------\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "    # 1) IBOVESPA (índice \"puro\", não-ETF)\n",
    "    sym_ibov = \"^BVSP\"\n",
    "    df_hist_ibov, raw_ibov, int_ibov = fetch_index_with_fallback(sym_ibov, intervals=(\"3mo\",\"1mo\",\"1d\"), min_rows=10)\n",
    "    save_raw_json(sym_ibov, int_ibov, raw_ibov)\n",
    "    save_quarterly_csv(sym_ibov, to_quarterly(df_hist_ibov, int_ibov))\n",
    "\n",
    "    # 2) IFIX (índice de FIIs da B3). Yahoo/BRAPI usam IFIX.SA\n",
    "    sym_ifix = \"IFIX.SA\"\n",
    "    df_hist_ifix, raw_ifix, int_ifix = fetch_index_with_fallback(sym_ifix, intervals=(\"3mo\",\"1mo\",\"1d\"), min_rows=10)\n",
    "    save_raw_json(sym_ifix, int_ifix, raw_ifix)\n",
    "    save_quarterly_csv(sym_ifix, to_quarterly(df_hist_ifix, int_ifix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# CONFIGURAÇÕES\n",
    "# =========================\n",
    "ARQ_BASE = \"base_para_simulador_indicadores_refatorado.csv\"\n",
    "SAIDA_RELATORIO = \"auditoria_simulador.xlsx\"\n",
    "SAIDA_BASE_CORRIGIDA = \"base_para_simulador_corrigida.csv\"\n",
    "APLICAR_CORRECOES = False  # deixe False para apenas diagnosticar; True para salvar versão corrigida\n",
    "\n",
    "# Colunas-chave (não numéricas)\n",
    "CHAVES = [\"Ticker\", \"Setor_Oficial\", \"Data_Referencia\"]\n",
    "\n",
    "# Regras de \"fallback\" (somente diagnóstico, a menos que APLICAR_CORRECOES=True)\n",
    "# Cada regra: primary será preenchido (apenas onde estiver NaN) pelo primeiro fallback disponível e válido.\n",
    "FALLBACK_REGRAS = [\n",
    "   # {\"primary\": \"EBIT\", \"fallbacks\": [\"Lucro_Antes_Impostos\", \"Lucro_Operacional\"]},\n",
    "    #{\"primary\": \"Receita_Liquida\", \"fallbacks\": [\"Receita_Total\"]},\n",
    "    # Se você tiver \"Divida_Bruta\" na base, pode habilitar a regra abaixo:\n",
    "    # {\"primary\": \"Divida_Bruta\", \"fallbacks\": [\"Divida_Total_Completa\", \"Emprestimos_Financiamentos\"]},\n",
    "]\n",
    "\n",
    "# Colunas deriváveis (somente diagnóstico por padrão)\n",
    "DERIVAVEIS = [\n",
    "    {\"dest\": \"Margem_Liquida\", \"formula\": lambda df: df[\"Lucro_Liquido\"] / df[\"Receita_Liquida\"], \"requisitos\": [\"Lucro_Liquido\", \"Receita_Liquida\"]},\n",
    "    {\"dest\": \"EV_EBITDA\", \"formula\": lambda df: df[\"Valor_Empresa\"] / df[\"EBITDA\"], \"requisitos\": [\"Valor_Empresa\", \"EBITDA\"]},\n",
    "    {\"dest\": \"Divida_Bruta_PL\", \"formula\": lambda df: df[\"Divida_Bruta\"] / df[\"Patrimonio_Liquido\"], \"requisitos\": [\"Divida_Bruta\", \"Patrimonio_Liquido\"]},\n",
    "]\n",
    "\n",
    "# Colunas esperadas numéricas (convertemos e auditamos tipo/faixa)\n",
    "# Ajuste conforme sua base; coloque aqui as colunas que deveriam ser numéricas\n",
    "COLS_NUMERICAS_EXPECTED = [\n",
    "    'Receita_Total','Receita_Liquida','Lucro_Operacional','Lucro_Liquido','Lucro_Antes_Impostos','EBIT',\n",
    "    'Despesas_Juros','Receita_Financeira','Despesa_Financeira','Resultado_Equivalencia_Patrimonial',\n",
    "    'Ativo_Total_Calculado','Ativo_Total_Completo','Ativo_Circulante','Imobilizado','Estoque',\n",
    "    'Recebiveis_Liquidos','Tributos_a_Recuperar','Outros_Ativos_Correntes',\n",
    "    'Caixa_e_Investimentos_Completo','Caixa_Gerado_Operacoes','Fluxo_Caixa_Operacional','Caixa_Final',\n",
    "    'Divida_Long_Prazo','Emprestimos_Financiamentos','Emprestimos_Financiamentos_Long_Prazo',\n",
    "    'Divida_Total_Completa','Debentures','Divida_Bruta_PL','Patrimonio_Liquido_Completo',\n",
    "    'Capital_Social_Completo','Lucros_Acumulados_Completo','Reservas_Completo','Ajustes_Avaliacao_Patrimonial',\n",
    "    'ROE','ROA','Margem_Liquida','EV_EBITDA','Lucro_Por_Acao','Preco_PVP','Preco_Lucro','Dividend_Yield',\n",
    "    'Valor_Empresa','Divida_Bruta','Patrimonio_Liquido','Preco_Atual'\n",
    "]\n",
    "\n",
    "# Faixas lógicas para checagens simples (opcional)\n",
    "FAIXAS_ESPERADAS = {\n",
    "    \"Margem_Liquida\": (-1.0, 1.0),  # -100% a 100%\n",
    "    \"ROE\": (-1.0, 2.0),             # permita outliers, ajuste se necessário\n",
    "    \"ROA\": (-1.0, 2.0),\n",
    "    \"EV_EBITDA\": (-1000, 1000),     # apenas para marcar extremos\n",
    "    \"Preco_PVP\": (0, 100),\n",
    "    \"Preco_Lucro\": (-1000, 1000),\n",
    "    \"Dividend_Yield\": (-1.0, 5.0),  # -100% a 500% (para evitar marcação indevida)\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# CARREGAR BASE\n",
    "# =========================\n",
    "df = pd.read_csv(ARQ_BASE)\n",
    "\n",
    "# =========================\n",
    "# CONVERSÕES DE TIPO\n",
    "# =========================\n",
    "# Forçar numéricas onde esperado (sem quebrar caso não exista)\n",
    "for col in COLS_NUMERICAS_EXPECTED:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# =========================\n",
    "# NULOS GERAIS E POR SETOR\n",
    "# =========================\n",
    "nulos_gerais = df.isna().mean().rename(\"Pct_Nulos\").to_frame().sort_values(\"Pct_Nulos\", ascending=False)\n",
    "\n",
    "if \"Setor_Oficial\" in df.columns:\n",
    "    nulos_por_setor = (\n",
    "        df.groupby(\"Setor_Oficial\")\n",
    "          .apply(lambda g: g.isna().mean())\n",
    "          .transpose()\n",
    "    )\n",
    "else:\n",
    "    nulos_por_setor = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# DIAGNÓSTICO DE FALLBACKS\n",
    "# =========================\n",
    "def diagnosticar_fallbacks(df, regras):\n",
    "    linhas = []\n",
    "    for regra in regras:\n",
    "        primary = regra[\"primary\"]\n",
    "        fallbacks = regra[\"fallbacks\"]\n",
    "        if primary not in df.columns:\n",
    "            linhas.append({\"Primary\": primary, \"Fallback_Usado\": None, \"OpLinhas_Preenchiveis\": 0, \"Observacao\": \"Primary inexistente na base\"})\n",
    "            continue\n",
    "\n",
    "        # Onde o primary está NaN\n",
    "        mask_primary_nan = df[primary].isna()\n",
    "        op_total = int(mask_primary_nan.sum())\n",
    "\n",
    "        fallback_usado = None\n",
    "        op_preenchiveis = 0\n",
    "        for fb in fallbacks:\n",
    "            if fb in df.columns:\n",
    "                candidatos = mask_primary_nan & df[fb].notna()\n",
    "                qtd = int(candidatos.sum())\n",
    "                if qtd > 0:\n",
    "                    fallback_usado = fb if fallback_usado is None else f\"{fallback_usado} | {fb}\"\n",
    "                    op_preenchiveis += qtd\n",
    "\n",
    "        obs = \"OK\" if op_total > 0 else \"Sem nulos em primary\"\n",
    "        linhas.append({\n",
    "            \"Primary\": primary,\n",
    "            \"Fallback_Usado\": fallback_usado,\n",
    "            \"OpLinhas_Preenchiveis\": op_preenchiveis,\n",
    "            \"Observacao\": obs\n",
    "        })\n",
    "    return pd.DataFrame(linhas)\n",
    "\n",
    "diag_fallbacks = diagnosticar_fallbacks(df, FALLBACK_REGRAS)\n",
    "\n",
    "# (Opcional) Aplicar correções de fato\n",
    "if APLICAR_CORRECOES:\n",
    "    for regra in FALLBACK_REGRAS:\n",
    "        primary = regra[\"primary\"]\n",
    "        if primary not in df.columns:\n",
    "            continue\n",
    "        mask_primary_nan = df[primary].isna()\n",
    "        for fb in regra[\"fallbacks\"]:\n",
    "            if fb in df.columns:\n",
    "                df.loc[mask_primary_nan & df[fb].notna(), primary] = df.loc[mask_primary_nan & df[fb].notna(), fb]\n",
    "                mask_primary_nan = df[primary].isna()  # atualiza máscara\n",
    "\n",
    "# =========================\n",
    "# DIAGNÓSTICO DERIVÁVEIS\n",
    "# =========================\n",
    "def diagnosticar_derivaveis(df, specs):\n",
    "    linhas = []\n",
    "    for spec in specs:\n",
    "        dest = spec[\"dest\"]\n",
    "        reqs = [r for r in spec[\"requisitos\"] if r in df.columns]\n",
    "        if len(reqs) < len(spec[\"requisitos\"]):\n",
    "            linhas.append({\"Coluna_Destino\": dest, \"Disponivel\": False, \"OpLinhas_Preenchiveis\": 0, \"Observacao\": \"Requisitos ausentes\"})\n",
    "            continue\n",
    "\n",
    "        # onde dest é NaN e todos requisitos não NaN\n",
    "        mask_dest_nan = ~df.columns.isin([dest]).any() or df[dest].isna()\n",
    "        if dest in df.columns:\n",
    "            mask_dest_nan = df[dest].isna()\n",
    "        else:\n",
    "            # se destino não existe, tratamos todos como \"preenchíveis em potencial\"\n",
    "            mask_dest_nan = pd.Series(True, index=df.index)\n",
    "\n",
    "        mask_reqs_ok = np.ones(len(df), dtype=bool)\n",
    "        for r in reqs:\n",
    "            mask_reqs_ok &= df[r].notna()\n",
    "        op_preenchiveis = int((mask_dest_nan & mask_reqs_ok).sum())\n",
    "\n",
    "        obs = \"OK\" if op_preenchiveis > 0 else \"Sem linhas preenchíveis\"\n",
    "        linhas.append({\"Coluna_Destino\": dest, \"Disponivel\": True, \"OpLinhas_Preenchiveis\": op_preenchiveis, \"Observacao\": obs})\n",
    "\n",
    "        # (Opcional) aplicar derivação\n",
    "        if APLICAR_CORRECOES and op_preenchiveis > 0:\n",
    "            try:\n",
    "                serie_calc = spec[\"formula\"](df)\n",
    "                if dest not in df.columns:\n",
    "                    df[dest] = np.nan\n",
    "                df.loc[mask_dest_nan & mask_reqs_ok, dest] = serie_calc[mask_dest_nan & mask_reqs_ok]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return pd.DataFrame(linhas)\n",
    "\n",
    "diag_derivaveis = diagnosticar_derivaveis(df, DERIVAVEIS)\n",
    "\n",
    "# =========================\n",
    "# ALERTAS DE TIPO/FAIXA\n",
    "# =========================\n",
    "alertas_tipo = []\n",
    "for col in COLS_NUMERICAS_EXPECTED:\n",
    "    if col in df.columns:\n",
    "        # quantos não numéricos viraram NaN\n",
    "        pct_nan = df[col].isna().mean()\n",
    "        alertas_tipo.append({\"Coluna\": col, \"Pct_NaN\": pct_nan})\n",
    "alertas_tipo = pd.DataFrame(alertas_tipo).sort_values(\"Pct_NaN\", ascending=False)\n",
    "\n",
    "alertas_faixa = []\n",
    "for col, (lo, hi) in FAIXAS_ESPERADAS.items():\n",
    "    if col in df.columns:\n",
    "        mask_out = df[col].notna() & ((df[col] < lo) | (df[col] > hi))\n",
    "        n_out = int(mask_out.sum())\n",
    "        if n_out > 0:\n",
    "            alertas_faixa.append({\"Coluna\": col, \"Qtd_Fora_Faixa\": n_out, \"Faixa_Esperada\": f\"[{lo}, {hi}]\"})\n",
    "alertas_faixa = pd.DataFrame(alertas_faixa).sort_values(\"Qtd_Fora_Faixa\", ascending=False) if alertas_faixa else pd.DataFrame(columns=[\"Coluna\",\"Qtd_Fora_Faixa\",\"Faixa_Esperada\"])\n",
    "\n",
    "# =========================\n",
    "# RELATÓRIO EXCEL\n",
    "# =========================\n",
    "with pd.ExcelWriter(SAIDA_RELATORIO, engine=\"xlsxwriter\") as w:\n",
    "    # Resumo geral\n",
    "    resumo = pd.DataFrame({\n",
    "        \"Linhas\": [len(df)],\n",
    "        \"Colunas\": [len(df.columns)],\n",
    "        \"Colunas_Num_Esperadas_Encontradas\": [sum([c in df.columns for c in COLS_NUMERICAS_EXPECTED])]\n",
    "    })\n",
    "    resumo.to_excel(w, sheet_name=\"00_Resumo\", index=False)\n",
    "\n",
    "    nulos_gerais.to_excel(w, sheet_name=\"01_Nulos_Gerais\")\n",
    "    if not nulos_por_setor.empty:\n",
    "        nulos_por_setor.to_excel(w, sheet_name=\"02_Nulos_por_Setor\")\n",
    "\n",
    "    diag_fallbacks.to_excel(w, sheet_name=\"03_Fallbacks\", index=False)\n",
    "    diag_derivaveis.to_excel(w, sheet_name=\"04_Derivaveis\", index=False)\n",
    "    alertas_tipo.to_excel(w, sheet_name=\"05_Alertas_Tipo\", index=False)\n",
    "    alertas_faixa.to_excel(w, sheet_name=\"06_Alertas_Faixa\", index=False)\n",
    "\n",
    "# =========================\n",
    "# SALVAR BASE CORRIGIDA (opcional)\n",
    "# =========================\n",
    "if APLICAR_CORRECOES:\n",
    "    df.to_csv(SAIDA_BASE_CORRIGIDA, index=False)\n",
    "    print(f\"Base corrigida salva em: {SAIDA_BASE_CORRIGIDA}\")\n",
    "\n",
    "print(f\"Relatório de auditoria salvo em: {SAIDA_RELATORIO}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
